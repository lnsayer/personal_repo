{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEIq/n/WnaHO4PB6Ry2l57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnsayer/personal_repo/blob/main/amazon_node_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon Dataset (from PyTorch Geometric) Node Classification Project"
      ],
      "metadata": {
        "id": "TjQq3HSS72s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Amazon Computers and Amazon Photo networks from the “Pitfalls of Graph Neural Network Evaluation” paper. Nodes represent goods and edges represent that two goods are frequently bought together. Given product reviews as bag-of-words node features, the task is to map goods to their respective product category."
      ],
      "metadata": {
        "id": "AsCf6sSx9diS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQf3Vnq49eEo",
        "outputId": "019b315a-acc3-4ccb-c50a-5f66364dd2eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Amazon\n",
        "import torch"
      ],
      "metadata": {
        "id": "5gaVXYgOBro3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Amazon(root=\"data/Amazon\", name=\"Computers\")\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print(f\"Number of nodes in graph is: {data.x.shape[0]}\")\n",
        "print(f\"Number of features for each node is {dataset.num_features}\")\n",
        "print(f\"Number of edges is {data.edge_index.shape[1]}\")\n",
        "print(f\"We want to predict the classes of the 13752 Computers\\n\")\n",
        "print(data, \"\\n\")\n",
        "\n",
        "print(f\"The number of class members for each class: {dataset.num_classes}\\n\")\n",
        "print(f\"Class labels: \\n{data.y.unique(return_counts=True)}\")\n",
        "#dir(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVgKCKir951L",
        "outputId": "a3a0870f-11ef-4bd1-ff5c-3d5a573165db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_computers.npz\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph is: 13752\n",
            "Number of features for each node is 767\n",
            "Number of edges is 491722\n",
            "We want to predict the classes of the 13752 Computers\n",
            "\n",
            "Data(x=[13752, 767], edge_index=[2, 491722], y=[13752]) \n",
            "\n",
            "The number of class members for each class: 10\n",
            "\n",
            "Class labels: \n",
            "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([ 436, 2142, 1414,  542, 5158,  308,  487,  818, 2156,  291]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will split the data using a transductive method in which the whole input graph is put through the forward method however\n",
        "# we split the labels (classes) for the loss function. We will use a split of train: 80%, val: 10% and test: 10%.\n",
        "torch.manual_seed(42)\n",
        "# indices of the nodes randomly shuffled\n",
        "indices = torch.randperm(data.x.shape[0])\n",
        "print(f\"Shuffled indices are {indices} of length {len(indices)}\\n\")\n",
        "\n",
        "# train:80%, val:10% and test:10%\n",
        "train_indices = indices[:int(0.8*len(indices))]\n",
        "val_indices = indices[int(0.8*len(indices)):int(0.9*len(indices))]\n",
        "test_indices = indices[int(0.9*len(indices)):]\n",
        "print(f\"Train shuffled indices are {train_indices} of length {len(train_indices)}\\n\")\n",
        "print(f\"Test shuffled indices are {test_indices} of length {len(test_indices)}\\n\")\n",
        "print(f\"Validation shuffled indices are {val_indices} of length {len(val_indices)}\\n\")\n",
        "\n",
        "# train labels to pass\n",
        "train_labels = data.y[train_indices]\n",
        "val_labels = data.y[val_indices]\n",
        "test_labels = data.y[test_indices]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecEGBWxvDz6a",
        "outputId": "e4520636-a23a-4ec2-b2fe-75ff5ad547be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffled indices are tensor([11094,  1027,  4378,  ...,  8275,  9941,  4338]) of length 13752\n",
            "\n",
            "Train shuffled indices are tensor([11094,  1027,  4378,  ...,  8049, 12835,    92]) of length 11001\n",
            "\n",
            "Test shuffled indices are tensor([8461, 5630, 8059,  ..., 8275, 9941, 4338]) of length 1376\n",
            "\n",
            "Validation shuffled indices are tensor([  794,  4772,  8150,  ..., 12435,   687,  5437]) of length 1375\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.is_undirected()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0SXDzTr-VYj",
        "outputId": "8729b8c7-e5f4-4c18-bd8e-023a31645468"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "nb_hidden_channels = 32\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = GCNConv(in_channels = dataset.num_features,\n",
        "                         out_channels = nb_hidden_channels)\n",
        "    self.conv2 = GCNConv(in_channels = nb_hidden_channels,\n",
        "                         out_channels = dataset.num_classes)\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    #x = F.dropout(x, training=self.training)\n",
        "    # could try log softmax: improved numerical performance and gradient optimisation\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "# defined it later too\n",
        "gcn_model = GCN()\n",
        "gcn_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4TVY2YKBl0k",
        "outputId": "7ac60f05-e851-4731-a846-dad2c1783b73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (conv1): GCNConv(767, 32)\n",
              "  (conv2): GCNConv(32, 10)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# set device as GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# instantiate the gcn_model\n",
        "gcn_model = GCN()\n",
        "# move the model to the gpu (or cpu is not available)\n",
        "gcn_model = gcn_model.to(device)\n",
        "\n",
        "optimiser = torch.optim.Adam(gcn_model.parameters(), lr=0.0001)\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "\n",
        "# define the data\n",
        "data = dataset[0]\n",
        "data = data.to(device)\n",
        "\n",
        "nb_epochs = 401\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "  # Train\n",
        "  # forward pass on the network\n",
        "  gcn_model.train()\n",
        "  out = gcn_model(data)\n",
        "\n",
        "  # calculate the loss\n",
        "  loss = loss_fn(out[train_indices], data.y[train_indices])\n",
        "\n",
        "  # calculate the label probabilities\n",
        "  label_probs = out[train_indices]\n",
        "  # calculate the label predictions\n",
        "  label_preds = torch.argmax(label_probs, dim=1)\n",
        "  # calculate accuracy\n",
        "  accuracy = (label_preds == data.y[train_indices]).sum()/len(label_preds)\n",
        "\n",
        "  # zero the gradients\n",
        "  optimiser.zero_grad()\n",
        "  # backpropagate the loss\n",
        "  loss.backward()\n",
        "  # update the optimizer\n",
        "  optimiser.step()\n",
        "\n",
        "  # Test\n",
        "  # Put the model in eval mode\n",
        "  gcn_model.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Calculate the loss for the test set\n",
        "    test_loss = loss_fn(out[test_indices], data.y[test_indices])\n",
        "\n",
        "    # Calculate the predictions for the test set and then calculate the accuracy\n",
        "    test_label_probs = out[test_indices]\n",
        "    test_label_preds = torch.argmax(test_label_probs, dim=1)\n",
        "    test_accuracy = (test_label_preds == data.y[test_indices]).sum()/len(test_label_preds)\n",
        "\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, Train Loss: {loss:.4f}, Train Accuracy: {accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9zxBu7-KX2a",
        "outputId": "b89aa700-70f7-41fe-a64d-7670690d6eab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: 2.4572, Train Accuracy: 0.1576, Test Loss: 2.4824, Test Accuracy: 0.1468\n",
            "Epoch: 10, Train Loss: 2.2600, Train Accuracy: 0.1591, Test Loss: 2.2749, Test Accuracy: 0.1475\n",
            "Epoch: 20, Train Loss: 2.1322, Train Accuracy: 0.3367, Test Loss: 2.1402, Test Accuracy: 0.3350\n",
            "Epoch: 30, Train Loss: 2.0551, Train Accuracy: 0.3744, Test Loss: 2.0607, Test Accuracy: 0.3576\n",
            "Epoch: 40, Train Loss: 1.9974, Train Accuracy: 0.3760, Test Loss: 2.0032, Test Accuracy: 0.3597\n",
            "Epoch: 50, Train Loss: 1.9516, Train Accuracy: 0.3841, Test Loss: 1.9578, Test Accuracy: 0.3692\n",
            "Epoch: 60, Train Loss: 1.9119, Train Accuracy: 0.3882, Test Loss: 1.9183, Test Accuracy: 0.3750\n",
            "Epoch: 70, Train Loss: 1.8754, Train Accuracy: 0.3951, Test Loss: 1.8817, Test Accuracy: 0.3837\n",
            "Epoch: 80, Train Loss: 1.8413, Train Accuracy: 0.4087, Test Loss: 1.8476, Test Accuracy: 0.3975\n",
            "Epoch: 90, Train Loss: 1.8091, Train Accuracy: 0.4311, Test Loss: 1.8159, Test Accuracy: 0.4172\n",
            "Epoch: 100, Train Loss: 1.7784, Train Accuracy: 0.4466, Test Loss: 1.7857, Test Accuracy: 0.4295\n",
            "Epoch: 110, Train Loss: 1.7488, Train Accuracy: 0.4712, Test Loss: 1.7565, Test Accuracy: 0.4564\n",
            "Epoch: 120, Train Loss: 1.7196, Train Accuracy: 0.5089, Test Loss: 1.7282, Test Accuracy: 0.4913\n",
            "Epoch: 130, Train Loss: 1.6885, Train Accuracy: 0.5558, Test Loss: 1.6964, Test Accuracy: 0.5320\n",
            "Epoch: 140, Train Loss: 1.6527, Train Accuracy: 0.5986, Test Loss: 1.6620, Test Accuracy: 0.5872\n",
            "Epoch: 150, Train Loss: 1.6185, Train Accuracy: 0.6089, Test Loss: 1.6284, Test Accuracy: 0.5930\n",
            "Epoch: 160, Train Loss: 1.5861, Train Accuracy: 0.6405, Test Loss: 1.5972, Test Accuracy: 0.6214\n",
            "Epoch: 170, Train Loss: 1.5555, Train Accuracy: 0.6508, Test Loss: 1.5668, Test Accuracy: 0.6323\n",
            "Epoch: 180, Train Loss: 1.5233, Train Accuracy: 0.6642, Test Loss: 1.5368, Test Accuracy: 0.6410\n",
            "Epoch: 190, Train Loss: 1.4939, Train Accuracy: 0.6792, Test Loss: 1.5072, Test Accuracy: 0.6570\n",
            "Epoch: 200, Train Loss: 1.4656, Train Accuracy: 0.6856, Test Loss: 1.4793, Test Accuracy: 0.6628\n",
            "Epoch: 210, Train Loss: 1.4385, Train Accuracy: 0.6888, Test Loss: 1.4522, Test Accuracy: 0.6679\n",
            "Epoch: 220, Train Loss: 1.4125, Train Accuracy: 0.6933, Test Loss: 1.4267, Test Accuracy: 0.6715\n",
            "Epoch: 230, Train Loss: 1.3862, Train Accuracy: 0.6963, Test Loss: 1.4007, Test Accuracy: 0.6759\n",
            "Epoch: 240, Train Loss: 1.3599, Train Accuracy: 0.7008, Test Loss: 1.3741, Test Accuracy: 0.6802\n",
            "Epoch: 250, Train Loss: 1.3334, Train Accuracy: 0.7036, Test Loss: 1.3479, Test Accuracy: 0.6831\n",
            "Epoch: 260, Train Loss: 1.3044, Train Accuracy: 0.7083, Test Loss: 1.3201, Test Accuracy: 0.6817\n",
            "Epoch: 270, Train Loss: 1.2775, Train Accuracy: 0.7107, Test Loss: 1.2929, Test Accuracy: 0.6882\n",
            "Epoch: 280, Train Loss: 1.2512, Train Accuracy: 0.7155, Test Loss: 1.2669, Test Accuracy: 0.6890\n",
            "Epoch: 290, Train Loss: 1.2263, Train Accuracy: 0.7194, Test Loss: 1.2419, Test Accuracy: 0.6933\n",
            "Epoch: 300, Train Loss: 1.2027, Train Accuracy: 0.7226, Test Loss: 1.2184, Test Accuracy: 0.6969\n",
            "Epoch: 310, Train Loss: 1.1803, Train Accuracy: 0.7269, Test Loss: 1.1961, Test Accuracy: 0.6984\n",
            "Epoch: 320, Train Loss: 1.1588, Train Accuracy: 0.7299, Test Loss: 1.1747, Test Accuracy: 0.7020\n",
            "Epoch: 330, Train Loss: 1.1383, Train Accuracy: 0.7327, Test Loss: 1.1543, Test Accuracy: 0.7035\n",
            "Epoch: 340, Train Loss: 1.1186, Train Accuracy: 0.7357, Test Loss: 1.1347, Test Accuracy: 0.7057\n",
            "Epoch: 350, Train Loss: 1.0996, Train Accuracy: 0.7378, Test Loss: 1.1159, Test Accuracy: 0.7115\n",
            "Epoch: 360, Train Loss: 1.0813, Train Accuracy: 0.7396, Test Loss: 1.0977, Test Accuracy: 0.7151\n",
            "Epoch: 370, Train Loss: 1.0636, Train Accuracy: 0.7411, Test Loss: 1.0802, Test Accuracy: 0.7158\n",
            "Epoch: 380, Train Loss: 1.0464, Train Accuracy: 0.7425, Test Loss: 1.0632, Test Accuracy: 0.7195\n",
            "Epoch: 390, Train Loss: 1.0296, Train Accuracy: 0.7447, Test Loss: 1.0465, Test Accuracy: 0.7217\n",
            "Epoch: 400, Train Loss: 1.0100, Train Accuracy: 0.7484, Test Loss: 1.0268, Test Accuracy: 0.7224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try out current method on Cora dataset"
      ],
      "metadata": {
        "id": "TjC2A84xMaY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid"
      ],
      "metadata": {
        "id": "QoRbzyVLo7jC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root=\"data/Planetoid\", name=\"Cora\")\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print(f\"Number of nodes in graph is: {data.x.shape[0]}\")\n",
        "print(f\"Number of features for each node is {dataset.num_features}\")\n",
        "print(f\"Number of edges is {data.edge_index.shape[1]}\")\n",
        "print(f\"We want to predict the classes of the 13752 papers\\n\")\n",
        "print(data, \"\\n\")\n",
        "\n",
        "print(f\"The number of class members for each class: {dataset.num_classes}\\n\")\n",
        "print(f\"Class labels: \\n{data.y.unique(return_counts=True)}\")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87-tktxPoyqD",
        "outputId": "412de63c-ec91-4892-ca5a-bd160edefbd0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph is: 2708\n",
            "Number of features for each node is 1433\n",
            "Number of edges is 10556\n",
            "We want to predict the classes of the 13752 papers\n",
            "\n",
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]) \n",
            "\n",
            "The number of class members for each class: 7\n",
            "\n",
            "Class labels: \n",
            "(tensor([0, 1, 2, 3, 4, 5, 6]), tensor([351, 217, 418, 818, 426, 298, 180]))\n",
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will split the data using a transductive method in which the whole input graph is put through the forward method however\n",
        "# we split the labels (classes) for the loss function. We will use a split of train: 80%, val: 10% and test: 10%.\n",
        "torch.manual_seed(42)\n",
        "# indices of the nodes randomly shuffled\n",
        "indices = torch.randperm(data.x.shape[0])\n",
        "print(f\"Shuffled indices are {indices} of length {len(indices)}\\n\")\n",
        "\n",
        "# train:80%, val:10% and test:10%\n",
        "train_indices = indices[:int(0.8*len(indices))]\n",
        "val_indices = indices[int(0.8*len(indices)):int(0.9*len(indices))]\n",
        "test_indices = indices[int(0.9*len(indices)):]\n",
        "print(f\"Train shuffled indices are {train_indices} of length {len(train_indices)}\\n\")\n",
        "print(f\"Test shuffled indices are {test_indices} of length {len(test_indices)}\\n\")\n",
        "print(f\"Validation shuffled indices are {val_indices} of length {len(val_indices)}\\n\")\n",
        "\n",
        "# train labels to pass\n",
        "train_labels = data.y[train_indices]\n",
        "val_labels = data.y[val_indices]\n",
        "test_labels = data.y[test_indices]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JlmAr07pAbw",
        "outputId": "254df2a0-83b3-40fc-a1ad-137e15337fc8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffled indices are tensor([1594,  519,  528,  ...,  547,   72, 1362]) of length 2708\n",
            "\n",
            "Train shuffled indices are tensor([1594,  519,  528,  ..., 2707, 1649, 1425]) of length 2166\n",
            "\n",
            "Test shuffled indices are tensor([1666, 2032, 2015,  977, 2299,  957, 2238,   21, 2256, 1506, 2241,  845,\n",
            "         763, 1188, 1961,  210,  593,  354, 2360,   67,  251, 2618,  993, 1090,\n",
            "           6, 2424, 1713, 1039, 1511,  849, 2596, 1939, 1917,  738,   41,   54,\n",
            "         290,  460,  243,  542,  240, 1113, 2270, 1282,  891,  351, 1815, 1189,\n",
            "         698, 1237, 1261, 1433, 2037, 1451, 1081,  684, 2161,  869,  180, 1861,\n",
            "         921,  249,  497, 2072, 1339,  604, 1185,   30, 2213,  166, 1577, 1022,\n",
            "        2487,  259, 2221, 1177,  434,  176, 2458, 1737,  403, 1762,    2,  373,\n",
            "        1557,  406, 1518,   37, 1636, 2373,  530, 1746,  967, 1120, 1629, 1694,\n",
            "         348, 1760, 2512, 1920, 1790,  376, 2412, 1748,  153, 1286, 1234, 2132,\n",
            "        1187,  561, 1822, 1463, 1517, 2008, 1608, 1681,  419, 2296, 1335, 1273,\n",
            "        2421, 1962, 1829, 1127,  285, 1859, 1969,  167, 1966,  543, 2000,  945,\n",
            "        1465, 2508, 2002, 2235, 2471, 1402, 1775,  997, 2678,  191,  701, 1837,\n",
            "        1656,  729, 2355, 1583,  482,  170, 1458, 1668, 1742,  825,  231, 1059,\n",
            "         556, 1874, 2342, 2164, 2685, 2460, 1990, 1478,  608, 1276, 2385, 1705,\n",
            "        1877, 2542,  538,  221,  457, 1880,  360, 1601,  544, 2602,   91, 1289,\n",
            "        1663,  232, 1643, 2223,  208, 1045, 1055, 1491,   96,  982, 2694,  441,\n",
            "         954,  332,  918, 2134, 1431,  960, 1988,  861, 2190,  162, 1929,  773,\n",
            "        2303, 1778,  775, 2643, 2146,  885,   90, 1819, 2231, 1010, 1565, 1943,\n",
            "         648, 1971,   86, 2636, 1900,  812,  266,  474,  489,  582, 1596, 2597,\n",
            "        1028, 1931, 1486, 1357,  663, 2511, 1083, 2377, 1399,  367,  428,  468,\n",
            "        1267, 2263, 1481, 2194,  318,  568,  598, 1586, 1400, 1300, 1641, 1585,\n",
            "        2671,  551,  722,  282, 1375, 1038, 1450, 2291,  425, 1536, 1392, 2547,\n",
            "        1496, 2183,  212, 2399,  547,   72, 1362]) of length 271\n",
            "\n",
            "Validation shuffled indices are tensor([1515, 1205, 1140, 1806,    8, 1293, 1412, 1418,  151, 1633, 1147, 2367,\n",
            "          60,  911, 2519,  614, 2658, 1278, 1015, 1271, 1495, 1393,  335,  610,\n",
            "         505,  733, 1181,  620, 1581, 1724, 2454, 1546, 2405, 2188,  996, 1317,\n",
            "         411, 2246, 2532,  759,  301, 1736, 2071,  649,   28, 2669, 2149, 1408,\n",
            "        2627, 1954, 2401, 1484, 2437,  179,  396, 2494,   87, 1490, 2703, 2526,\n",
            "        1977,  823,  715, 1054, 2309,  182,  494,  435,  341, 1470, 1216, 2348,\n",
            "         242,  502,  495,  537, 1710,  621, 2623,  739, 1692, 2443,   27,  914,\n",
            "        1764, 1005,  952, 1800,  855,  979,  856, 2380,  422, 1670, 1841, 1155,\n",
            "        1068, 1366, 1334, 2376, 1743, 1702, 2456, 1184, 2530,   49, 1255, 1182,\n",
            "        2090, 2140, 2579, 1488,  395,  152, 1777,  558, 2098, 1593,  287, 1564,\n",
            "        1125,  560, 1620,  647,  780, 2427,  989,  714,  426, 2431,  703,  912,\n",
            "         579, 2442, 1734, 1112, 1447, 1411, 1752, 2666,  642,  431, 2201,   38,\n",
            "         682,  513, 1168,  352, 1372, 1741,  377, 2261, 2036, 1729, 1285,  969,\n",
            "         362, 1249, 1949, 1302, 2455, 2653, 2374, 2515, 1958,   70, 2262,  776,\n",
            "         947,  293, 2630,  935, 2481, 2234,  545, 2397,  375, 1079,  674,  723,\n",
            "        1738, 2153, 1885, 2543, 1612, 2172, 2108, 1221,  225,  949, 2705, 1246,\n",
            "         138, 2586, 2551,  890, 2020, 2284, 2535, 1021, 2631,   73, 1934,  365,\n",
            "         304, 1818, 2403, 2084, 2425, 1329, 1728, 1419, 1031, 2362,  797, 2449,\n",
            "         893, 2531, 1222, 2280,  817,  702, 2126, 1659, 2571, 2013, 2563,   85,\n",
            "        1493,  627, 2237, 1582, 1208,  408, 1309, 1259, 1427,  445,  427, 1865,\n",
            "        1749,  247,  783,  801, 1597,  440, 1683, 1415,  303, 2414,  102,  541,\n",
            "         660, 1686, 2053, 2305,  211, 2419,  985,  899,  273, 1609, 1065, 1693,\n",
            "         132, 2318,  800,  573, 1974, 1552, 1142]) of length 271\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.is_undirected()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uciebv8pbuO",
        "outputId": "fce5eff1-21a8-4035-8f52-aaa320e02deb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "nb_hidden_channels = 32\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = GCNConv(in_channels = dataset.num_features,\n",
        "                         out_channels = nb_hidden_channels)\n",
        "    self.conv2 = GCNConv(in_channels = nb_hidden_channels,\n",
        "                         out_channels = dataset.num_classes)\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    #x = F.dropout(x, training=self.training)\n",
        "    # could try log softmax: improved numerical performance and gradient optimisation\n",
        "    x = torch.softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "# defined it later too\n",
        "gcn_model = GCN()\n",
        "gcn_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu-yUS__pcs8",
        "outputId": "ea0f7c3f-0169-4ea3-ef4c-5fa69ca8dbf6"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (conv1): GCNConv(1433, 32)\n",
              "  (conv2): GCNConv(32, 7)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# set device as GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# instantiate the gcn_model\n",
        "gcn_model = GCN()\n",
        "# move the model to the gpu (or cpu is not available)\n",
        "gcn_model = gcn_model.to(device)\n",
        "\n",
        "optimiser = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# define the data\n",
        "data = dataset[0]\n",
        "data = data.to(device)\n",
        "\n",
        "nb_epochs = 200\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "  gcn_model.train()\n",
        "  # forward pass on the network\n",
        "  out = gcn_model(data)\n",
        "\n",
        "  # calculate the loss\n",
        "  loss = loss_fn(out[train_indices], data.y[train_indices])\n",
        "\n",
        "  # calculate the label probabilities\n",
        "  label_probs = out[torch.concat((val_indices, test_indices))]\n",
        "  # calculate the label predictions\n",
        "  label_preds = torch.argmax(label_probs, dim=1)\n",
        "  # calculate accuracy\n",
        "  accuracy = (label_preds == data.y[torch.concat((val_indices, test_indices))]).sum()/len(label_preds)\n",
        "\n",
        "  # zero the gradients\n",
        "  optimiser.zero_grad()\n",
        "  # backpropagate the loss\n",
        "  loss.backward()\n",
        "  # update the optimizer\n",
        "  optimiser.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bS5IgXvplLM",
        "outputId": "7d2facef-50f7-402f-8172-e299295c28d1"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.9462, Accuracy: 0.1052\n",
            "Epoch: 10, Loss: 1.5051, Accuracy: 0.7472\n",
            "Epoch: 20, Loss: 1.3360, Accuracy: 0.8155\n",
            "Epoch: 30, Loss: 1.2846, Accuracy: 0.8395\n",
            "Epoch: 40, Loss: 1.2463, Accuracy: 0.8801\n",
            "Epoch: 50, Loss: 1.2295, Accuracy: 0.8801\n",
            "Epoch: 60, Loss: 1.2216, Accuracy: 0.8782\n",
            "Epoch: 70, Loss: 1.2161, Accuracy: 0.8801\n",
            "Epoch: 80, Loss: 1.2127, Accuracy: 0.8727\n",
            "Epoch: 90, Loss: 1.2101, Accuracy: 0.8745\n",
            "Epoch: 100, Loss: 1.2072, Accuracy: 0.8764\n",
            "Epoch: 110, Loss: 1.2045, Accuracy: 0.8727\n",
            "Epoch: 120, Loss: 1.2024, Accuracy: 0.8745\n",
            "Epoch: 130, Loss: 1.2005, Accuracy: 0.8819\n",
            "Epoch: 140, Loss: 1.1992, Accuracy: 0.8801\n",
            "Epoch: 150, Loss: 1.1982, Accuracy: 0.8782\n",
            "Epoch: 160, Loss: 1.1975, Accuracy: 0.8782\n",
            "Epoch: 170, Loss: 1.1969, Accuracy: 0.8782\n",
            "Epoch: 180, Loss: 1.1965, Accuracy: 0.8801\n",
            "Epoch: 190, Loss: 1.1960, Accuracy: 0.8764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor(([1,2,3, 4, 5, 6], [7,8,9, 10, 11, 12]), dtype=torch.float)\n",
        "print(t,t.shape)\n",
        "\n",
        "print(torch.softmax(t, dim=0))\n",
        "print(torch.softmax(t, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9CAdjgUplfa",
        "outputId": "b6035aa7-fe7b-4236-a4de-362ef8319df0"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
            "        [ 7.,  8.,  9., 10., 11., 12.]]) torch.Size([2, 6])\n",
            "tensor([[0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025],\n",
            "        [0.9975, 0.9975, 0.9975, 0.9975, 0.9975, 0.9975]])\n",
            "tensor([[0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337],\n",
            "        [0.0043, 0.0116, 0.0315, 0.0858, 0.2331, 0.6337]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kt3xNbiqskWX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}