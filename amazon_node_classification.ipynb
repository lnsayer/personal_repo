{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCctIweIW4OlFItkgPX41g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnsayer/personal_repo/blob/main/amazon_node_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon Dataset (from PyTorch Geometric) Node Classification Project"
      ],
      "metadata": {
        "id": "TjQq3HSS72s4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Amazon Computers and Amazon Photo networks from the “Pitfalls of Graph Neural Network Evaluation” paper. Nodes represent goods and edges represent that two goods are frequently bought together. Given product reviews as bag-of-words node features, the task is to map goods to their respective product category."
      ],
      "metadata": {
        "id": "AsCf6sSx9diS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQf3Vnq49eEo",
        "outputId": "50889709-ac7b-46da-c4c6-2cbeac3142ec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Amazon\n",
        "import torch"
      ],
      "metadata": {
        "id": "5gaVXYgOBro3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Amazon(root=\"data/Amazon\", name=\"Computers\")\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print(f\"Number of nodes in graph is: {data.x.shape[0]}\")\n",
        "print(f\"Number of features for each node is {dataset.num_features}\")\n",
        "print(f\"Number of edges is {data.edge_index.shape[1]}\")\n",
        "print(f\"We want to predict the classes of the 13752 Computers\\n\")\n",
        "print(data, \"\\n\")\n",
        "\n",
        "print(f\"The number of class members for each class: {dataset.num_classes}\\n\")\n",
        "print(f\"Class labels: \\n{data.y.unique(return_counts=True)}\")\n",
        "#dir(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVgKCKir951L",
        "outputId": "ca1e1bd2-e81a-45fe-ba10-dab2b5ce6d92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph is: 13752\n",
            "Number of features for each node is 767\n",
            "Number of edges is 491722\n",
            "We want to predict the classes of the 13752 Computers\n",
            "\n",
            "Data(x=[13752, 767], edge_index=[2, 491722], y=[13752]) \n",
            "\n",
            "The number of class members for each class: 10\n",
            "\n",
            "Class labels: \n",
            "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([ 436, 2142, 1414,  542, 5158,  308,  487,  818, 2156,  291]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will split the data using a transductive method in which the whole input graph is put through the forward method however\n",
        "# we split the labels (classes) for the loss function. We will use a split of train:80%, val:10% and test:10%.\n",
        "torch.manual_seed(42)\n",
        "# indices of the nodes randomly shuffled\n",
        "indices = torch.randperm(data.x.shape[0])\n",
        "\n",
        "# train:80%, val:10% and test:10%\n",
        "train_indices = indices[:int(0.8*len(indices))]\n",
        "val_indices = indices[int(0.8*len(indices)):int(0.9*len(indices))]\n",
        "test_indices = indices[int(0.9*len(indices)):]\n",
        "\n",
        "# train labels to pass\n",
        "train_labels = data.y[train_indices]\n",
        "val_labels = data.y[val_indices]\n",
        "test_labels = data.y[test_indices]\n",
        "\n",
        "print(len(val_labels), len(test_labels))\n",
        "print(torch.concat((val_indices, test_indices)))\n",
        "\n",
        "print(len(indices))\n",
        "print(len(train_indices))\n",
        "print(len(val_indices))\n",
        "print(len(test_indices))\n",
        "\n",
        "print(test_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecEGBWxvDz6a",
        "outputId": "1ac6b7ed-3e3c-4b6b-f1f7-5245b8c0e602"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 1000\n",
            "tensor([  794,  4772,  8150,  ...,  1300, 12066,  3247])\n",
            "13752\n",
            "1000\n",
            "1000\n",
            "1000\n",
            "tensor([ 8461,  5630,  8059,  1094,    45,  4962,  3510,  2548,  8251,  2735,\n",
            "        12073, 12921,  5425,  7690,  9579,  8783,   468,  8391,  7467, 12950,\n",
            "         9889,  7624,  6818,  4982, 13219,  8313,   159,  7823,  3233,  7636,\n",
            "         1059,  6132,  7355, 12230,  3646, 12507,  4793,  9586,  3596,  3545,\n",
            "          708, 10637, 10571,  2364, 12553,  3423,  6406,  6781,    50, 11683,\n",
            "         8594, 12739,  1681,  1037,  3394,  5460, 12677,  5709,  1391,   609,\n",
            "        12314,  6983,  4344,  2405,  6119,  3288,  9898,  1450,  4787,  9383,\n",
            "         4511,  6043, 10462,  5456,  2263,  8778,  2783,  2382,  9099, 11221,\n",
            "         8662, 11316,  7293,  9531,   653,  8405, 12335,  2588,   615,  8975,\n",
            "         1421, 10101, 12325,   946,  3710,  8048,  7367,  4064,  3934, 11613,\n",
            "          538,  6997,  6683,  7627,  7965,  4099,  5085,  5734,  9673,  4437,\n",
            "         9029, 12068,   734,  6853, 13191, 10714, 10816,  1404,  9633,  3617,\n",
            "         4399, 10346, 10748,  3335,   168,  2711,  1001, 11476,  5399,  4087,\n",
            "        11731, 11471,  7294, 12345,  6831,  9371, 13606,  8532,  1935,  6031,\n",
            "         6516, 12269,  1159, 11601,  9745, 10607,  8611,  4108,   595,  5761,\n",
            "         4584,  6592, 10255,   630,   355, 10044,  5126,  8467,  8587,  3399,\n",
            "        12575,  2956, 11691, 12410,  9730,  9439,  2940,  4666, 11523,  9997,\n",
            "         5489,  5625,    37,  2800,  2388,  8602, 12604,  7494,  4209,  5268,\n",
            "         4036,  5444,   286,  8818,  1493,  9003,  6575,  8074,  3000,   284,\n",
            "        13550,  2659,  2611, 11132, 12607,   803,  4401,    53, 12827,  8197,\n",
            "         7166,  1564,  2755,  1656,  5560,   636,  3983, 13034,   427,  4393,\n",
            "         4518,  6631,  2872, 12705,  5494,  9902, 10187,  2492,  8696,  9988,\n",
            "         7382,  5649,  8028,  8784,  9615, 10765, 11956, 12297, 13050,  4535,\n",
            "        10107,   116,  3187,  1267,  8574,  8047,  6370, 12120, 10860,    70,\n",
            "          612, 13051,   547,  5641,  9272, 12987,  4976,  7559, 12723, 12137,\n",
            "          899,  3531,  9216,  1088, 13697, 13698,  7364,  5082,  2471, 13721,\n",
            "         5870,  1002,  4039, 10830, 11835,  4550,  1289,  2172,  8399,  9235,\n",
            "         8043, 11219, 11502, 12952,  7319, 13347, 11575, 13601,  9484, 10805,\n",
            "        11941, 13040, 11609,  7689, 10776,  4113,  6307,   733,  1764,  3039,\n",
            "         4436,  3312,  5583,  5692,  2686,  2820, 10756, 11026,  6794, 10624,\n",
            "        10046,  7252,  3375,  9004,  1103,  7877,  5081,  1972,  7699,  6281,\n",
            "         1162,  2853,   446,  1441,  1337,  6279,  9537, 13486,  7472,  4871,\n",
            "         4684,  1774, 10289,  4413,  2727,  3381,  6409,  2170, 13639,  9387,\n",
            "         3001,   601,  7490,  8257, 11937,  4536,  9330,  9393,   757,  2958,\n",
            "         7553,  2478, 10163,  5528,  1089, 11449,  2888,  6060,  3259,  1084,\n",
            "         5492,  1892, 12186,  7794, 11015, 10695,  5940, 11806,  4069,  5394,\n",
            "         3253,   777, 11098, 12447, 11003,  7744, 12483,  4657,  5238,  2950,\n",
            "         5180,  9087,  1378,  2130,  1036, 11962, 12497, 12062,  3750,  2482,\n",
            "        11542,  4692, 13298, 13370,  6655,  5311,  5992,  6160,  2610,  6286,\n",
            "         9839,  9751, 11050, 11591, 13349, 11922,  8088,   854,  6870,  7462,\n",
            "         2029, 12911, 12573,  8266, 10276, 12828,  8276,  5956,  4896,  5474,\n",
            "         6009,  5872,  6993, 10570,  8507,  5886,  8303,  2868,   200,  5605,\n",
            "        11483,  9938,  1465,   367, 11118,  2643,  9430,  7389,  7623,  1291,\n",
            "          582, 12737,  8326, 12999,  4214,  2331,  6752,  2152,  9215, 10731,\n",
            "          358,  6360,  9275,  7099, 11109,  1343,  7692,  4421, 10284,  4464,\n",
            "        13164,   821,  1242,   277,  1957,  2078,  2645, 13525,  7457,  4255,\n",
            "         7530,  1766,  1050,  5066,  7730,  9475,  7232, 10050, 10404, 13322,\n",
            "          698,  5694,  4537, 10091,   877,  3804, 12477,  7500,   564,  4261,\n",
            "         7157,  5972, 13233,  1131,  8446,  9259, 12255, 10649,  5275,  1023,\n",
            "         3628,  4293,  1581,  9030,  4711,  5976,  6344, 12232, 13190,  8055,\n",
            "         4931,  7120, 10582, 12923,  2329,  1998,  5655,  8988,  5652,  4201,\n",
            "         5707,  5352,  1198,  2596,  1210,   750,    35, 11912,  7048, 10726,\n",
            "          715, 11413, 10043,  2128,   280,  4477,  9176,  3122, 13496,  1940,\n",
            "         1092,  1660, 12369,  9735,  2941,  1786,  9689,  2665, 11294,  5495,\n",
            "         4400,  5824, 12695, 12857, 11570,  1408,  3623,  6431,  3793,  6680,\n",
            "         5260,  3075, 10551,  2743, 13091,  3424,  8793,   102, 11926, 12252,\n",
            "         3851, 13271,  9141, 11785,  7572,  8167,  4613,   914,   649,  4415,\n",
            "         1217,   916,  9301, 12925,  8159,  1946,  2367,  7450, 10278,  4745,\n",
            "         9935,  8742,  3968,  7234,  4359,  5291,  7019,   174,   344,  7482,\n",
            "         6029, 12089,  1867,  7250, 12572,  2294,  8208,   126,  8877, 11569,\n",
            "         5645,  9264,  2966, 12268,  1157, 11009, 12278,  6436, 13605,  4286,\n",
            "         5331,   705,  7283,  4680,  7875,  8083,  1927,   713, 10348,  3174,\n",
            "         8432,  1179,  7091,  9076,  8129,   252, 12936,  5736, 12302,    51,\n",
            "         6588, 13479, 10511,  2593,  6943, 13459,  3308,  9121,  9809,  5343,\n",
            "        10769,  6742,  1354,   334,  8836,   586, 10188,  4454,  5868,  8555,\n",
            "         8837,  7489,  8798,  3897,  2622,  8024, 12924,  3485,  3824, 12037,\n",
            "         6425, 13053,  8884,  6052, 12609,  4587,  8897,  1132, 11855, 11236,\n",
            "        12546,  1920, 12764, 11334,  7216,  7291, 10532, 12773,  4167,  4823,\n",
            "         2898,  9820,  2623, 10500,  6847,  6176, 10442,  7798,  2896,  3064,\n",
            "        10550,  6367,  5459,  6963,  1529,  3080,  4890, 11275,  4922,  6598,\n",
            "         4644,  1567,  4677,  4826,  1768,  7363,  9197,  9642,  2525,  6333,\n",
            "         1765,  4029, 13526,  4071,  4005,  5728, 12102, 10103,  9696,  5623,\n",
            "         6450,     3,  5631, 11896,  8717, 11521,  3402,  5769,  3837,  1580,\n",
            "         3469,   373,  7175,  6017, 13437,  8789,  9458,  3881,  2461,  7285,\n",
            "        13660,   546,  8489,  2549, 13105,  3563, 11056,   846,  4022,  4610,\n",
            "          581,  2299,  8285,  6334,  4067,  7980,  6010,  3714, 12026, 11110,\n",
            "         2994,  1770, 10062, 11263,  2948,  1412,  2748,  8995,  7189,  9593,\n",
            "         7723,  8641,  7808, 12432,  4725,  2689,  6094, 13464,  1018,  8162,\n",
            "        12534,  2580,  6224, 10662,  9102,  9660,   570, 13346, 13594,  7908,\n",
            "         4200, 13009,  7079,  3619,  8025,  4901,  6798,  7973, 11929,  6855,\n",
            "         4864, 11327,  2127,  4361,  9245,  9879,  1321,  6821,  2328,  4358,\n",
            "         3162,  3373,  4235,  4688,  4516, 13726,  5805,  8702, 13733,  5017,\n",
            "          692,  9679, 12605, 12975, 12370,  6247,  3051, 10938, 12645,  7932,\n",
            "         9977,  1569, 11985, 13679,  4673,  4636,   166, 12075,  3564,  4771,\n",
            "         7438,  3847, 11422,  5781,  3114,  2827,  7638,  4665,  4439,  7493,\n",
            "         7052,  2514,  9169, 10345,  9706,    23,  8147, 10260,  6546,  1201,\n",
            "        12129,  9699,  1787, 12555,  4019,  1127, 10544,  5117, 10378, 11112,\n",
            "         1626,  2905,  6921,  7945,  2352,  7978,  9016,  4494,  6215,  1639,\n",
            "         3530,  2740,   792, 11938,  3902, 10300,  7041, 13598, 12558,  6644,\n",
            "         5318,  9236,  5944,  3504,  9123,  4196,  7802,  9293, 11726,  1895,\n",
            "         1070,  3988,  2942,  6195,  8692, 11647,  1937,  4031,  9365,  1055,\n",
            "         7743,  8460, 13174,  9682,  7639,  9389, 11028,  8192, 12660,  7625,\n",
            "         4133,  1850,  5279,  3358,  5787, 12906,   995,  3695, 12807, 12380,\n",
            "        12220,  3395,  8099,  8699,  3687,  8733, 10143, 12107,  7130, 12470,\n",
            "         3243,  6704,  3371,   752,  1090,  8878,  4580,  4582, 13030,  3591,\n",
            "         2237,  6764,  9858,  7992,  3629,  8304,   205, 13139,  9414,  6461,\n",
            "         5934,  9568,  8324,   621, 12494,  8084,  7071,  9062,  7626, 11424,\n",
            "         1555,  4707, 12157,  1335, 13246, 13450, 11445,  7889,  4110,  7246,\n",
            "        11383,  3449,  3329,  3501,  4626,   222,  3939, 13218,  8148,  9094,\n",
            "         7417,  4325,  6604,  5561,  3154,  4059,  1247,  1300, 12066,  3247])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.is_undirected()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0SXDzTr-VYj",
        "outputId": "7be011ee-e2eb-463d-f1f2-e9f1ad796431"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "nb_hidden_channels = 32\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = GCNConv(in_channels = dataset.num_features,\n",
        "                         out_channels = nb_hidden_channels)\n",
        "    self.conv2 = GCNConv(in_channels = nb_hidden_channels,\n",
        "                         out_channels = dataset.num_classes)\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    # could try log softmax: improved numerical performance and gradient optimisation\n",
        "    x = F.log_softmax(x, dim = 1)\n",
        "\n",
        "    return x\n",
        "\n",
        "# defined it later too\n",
        "gcn_model = GCN()\n",
        "gcn_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4TVY2YKBl0k",
        "outputId": "410ead94-5dc1-4381-9606-ec30e00c47a8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (conv1): GCNConv(767, 32)\n",
              "  (conv2): GCNConv(32, 10)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = torch.optim.Adam(gcn_model.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ecabmS3lIo74"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "\n",
        "# set device as GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# instantiate the gcn_model\n",
        "gcn_model = GCN()\n",
        "# move the model to the gpu (or cpu is not available)\n",
        "gcn_model = gcn_model.to(device)\n",
        "\n",
        "# define the data\n",
        "data = dataset[0]\n",
        "data = data.to(device)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "nb_epochs = 500\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "  # forward pass on the network\n",
        "  out = gcn_model(data)\n",
        "\n",
        "  # calculate the loss\n",
        "  loss = loss_fn(out[train_indices], data.y[train_indices])\n",
        "\n",
        "  # calculate the label probabilities\n",
        "  label_probs = out[torch.concat((val_indices, test_indices))]\n",
        "  # calculate the label predictions\n",
        "  label_preds = torch.argmax(label_probs, dim=1)\n",
        "  # calculate accuracy\n",
        "  accuracy = (label_preds == data.y[torch.concat((val_indices, test_indices))]).sum()/len(label_preds)\n",
        "\n",
        "  # zero the gradients\n",
        "  optimiser.zero_grad()\n",
        "  # backpropagate the loss\n",
        "  loss.backward()\n",
        "  # update the optimizer\n",
        "  optimiser.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9zxBu7-KX2a",
        "outputId": "a24010fb-0c33-45ed-ad2d-dbcca9b301ce"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.6151, Accuracy: 0.1178\n",
            "Epoch: 10, Loss: 2.6223, Accuracy: 0.1105\n",
            "Epoch: 20, Loss: 2.6234, Accuracy: 0.1203\n",
            "Epoch: 30, Loss: 2.6243, Accuracy: 0.1203\n",
            "Epoch: 40, Loss: 2.6141, Accuracy: 0.1087\n",
            "Epoch: 50, Loss: 2.6304, Accuracy: 0.1116\n",
            "Epoch: 60, Loss: 2.6267, Accuracy: 0.1170\n",
            "Epoch: 70, Loss: 2.6329, Accuracy: 0.1123\n",
            "Epoch: 80, Loss: 2.6239, Accuracy: 0.1160\n",
            "Epoch: 90, Loss: 2.6299, Accuracy: 0.1134\n",
            "Epoch: 100, Loss: 2.6117, Accuracy: 0.1163\n",
            "Epoch: 110, Loss: 2.6253, Accuracy: 0.1250\n",
            "Epoch: 120, Loss: 2.6206, Accuracy: 0.1160\n",
            "Epoch: 130, Loss: 2.6196, Accuracy: 0.1221\n",
            "Epoch: 140, Loss: 2.6209, Accuracy: 0.1203\n",
            "Epoch: 150, Loss: 2.6202, Accuracy: 0.1174\n",
            "Epoch: 160, Loss: 2.6239, Accuracy: 0.1160\n",
            "Epoch: 170, Loss: 2.6189, Accuracy: 0.1098\n",
            "Epoch: 180, Loss: 2.6200, Accuracy: 0.1174\n",
            "Epoch: 190, Loss: 2.6163, Accuracy: 0.1170\n",
            "Epoch: 200, Loss: 2.6292, Accuracy: 0.1185\n",
            "Epoch: 210, Loss: 2.6217, Accuracy: 0.1221\n",
            "Epoch: 220, Loss: 2.6225, Accuracy: 0.1156\n",
            "Epoch: 230, Loss: 2.6315, Accuracy: 0.1130\n",
            "Epoch: 240, Loss: 2.6103, Accuracy: 0.1054\n",
            "Epoch: 250, Loss: 2.6179, Accuracy: 0.1094\n",
            "Epoch: 260, Loss: 2.6193, Accuracy: 0.1091\n",
            "Epoch: 270, Loss: 2.6251, Accuracy: 0.1138\n",
            "Epoch: 280, Loss: 2.6248, Accuracy: 0.1192\n",
            "Epoch: 290, Loss: 2.6277, Accuracy: 0.1200\n",
            "Epoch: 300, Loss: 2.6231, Accuracy: 0.1130\n",
            "Epoch: 310, Loss: 2.6259, Accuracy: 0.1178\n",
            "Epoch: 320, Loss: 2.6209, Accuracy: 0.1156\n",
            "Epoch: 330, Loss: 2.6173, Accuracy: 0.1141\n",
            "Epoch: 340, Loss: 2.6250, Accuracy: 0.1221\n",
            "Epoch: 350, Loss: 2.6242, Accuracy: 0.1120\n",
            "Epoch: 360, Loss: 2.6244, Accuracy: 0.1163\n",
            "Epoch: 370, Loss: 2.6196, Accuracy: 0.1130\n",
            "Epoch: 380, Loss: 2.6078, Accuracy: 0.1120\n",
            "Epoch: 390, Loss: 2.6256, Accuracy: 0.1207\n",
            "Epoch: 400, Loss: 2.6193, Accuracy: 0.1156\n",
            "Epoch: 410, Loss: 2.6197, Accuracy: 0.1141\n",
            "Epoch: 420, Loss: 2.6158, Accuracy: 0.1178\n",
            "Epoch: 430, Loss: 2.6195, Accuracy: 0.1174\n",
            "Epoch: 440, Loss: 2.6251, Accuracy: 0.1236\n",
            "Epoch: 450, Loss: 2.6213, Accuracy: 0.1160\n",
            "Epoch: 460, Loss: 2.6102, Accuracy: 0.1258\n",
            "Epoch: 470, Loss: 2.6347, Accuracy: 0.1130\n",
            "Epoch: 480, Loss: 2.6301, Accuracy: 0.1149\n",
            "Epoch: 490, Loss: 2.6170, Accuracy: 0.1101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TjC2A84xMaY_"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}