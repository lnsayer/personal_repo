{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxnNNjzgC9+hj8HdhhnTIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnsayer/personal_repo/blob/main/drug%20discovery%20with%20BACE%20dataset/bace_dataset_going_modular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PMC5MYEY5IHv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import Javascript\n",
        "\n",
        "\n",
        "os.makedirs(\"going_modular\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALVpB0_k6vaw",
        "outputId": "53c11b6f-fe10-4410-ec74-04f5665b17fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "\n",
        "!pip install torch_geometric\n",
        "!pip install deepchem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Oo78YaqZgE-w",
        "outputId": "317f0f06-c597-402e-d349-a25eb4c5c675"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.13.0)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.11.4)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2024.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2024.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->deepchem) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from deepchem.feat.graph_data import GraphData\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Suppress TensorFlow logs\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Redirect standard error output\n",
        "sys.stderr = open(os.devnull, 'w')\n",
        "\n",
        "# Configure logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "x5FMkYoxjxBR"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get_data.py"
      ],
      "metadata": {
        "id": "2njAx5rDHzDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/get_data.py\n",
        "from timeit import default_timer as timer\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "start_time()\n",
        "\n",
        "# If want to download bace csv straight into colab\n",
        "data_path = Path(\"going_modular/data/\")\n",
        "bace_path = data_path / \"raw\"\n",
        "\n",
        "if bace_path.is_dir():\n",
        "  print(f\"{bace_path} is already a directory\")\n",
        "else:\n",
        "  print(f\"{bace_path} is not a directory, creating one\")\n",
        "  bace_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  with open(bace_path / \"bace.csv\", \"wb\") as f:\n",
        "    request = requests.get(\"https://raw.githubusercontent.com/lnsayer/personal_repo/main/drug%20discovery%20with%20BACE%20dataset/data/bace.csv\")\n",
        "    print(\"Downloading data\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Resave the csv files without unnecessary columns\n",
        "bace_df = pd.read_csv(bace_path/ \"bace.csv\")\n",
        "bace_df = bace_df[[\"mol\", \"CID\", \"Class\", \"Model\", \"pIC50\"]]\n",
        "bace_df.to_csv(bace_path/\"bace.csv\")\n",
        "\n",
        "bace_df\n",
        "end_time()\n",
        "print(f\"get_data took {end_time - start_time:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgG-FxWA6-GR",
        "outputId": "7e7cb7c2-7978-42c7-aa32-e4c0947bf1de"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/get_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/get_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sgIBmVX7H5d",
        "outputId": "9a93c8f6-f400-4d99-ddc5-8acf13ef8b9f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "going_modular/data/raw is already a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "bace_df = pd.read_csv(\"going_modular/data/raw/bace.csv\")\n",
        "bace_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LBnkj2757mv4",
        "outputId": "07f53da4-73a1-45b5-9f42-b06ab1377762"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0                                                mol  \\\n",
              "0              0  O1CC[C@@H](NC(=O)[C@@H](Cc2cc3cc(ccc3nc2N)-c2c...   \n",
              "1              1  Fc1cc(cc(F)c1)C[C@H](NC(=O)[C@@H](N1CC[C@](NC(...   \n",
              "2              2  S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...   \n",
              "3              3  S1(=O)(=O)C[C@@H](Cc2cc(O[C@H](COCC)C(F)(F)F)c...   \n",
              "4              4  S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...   \n",
              "...          ...                                                ...   \n",
              "1508        1508          Clc1cc2nc(n(c2cc1)C(CC(=O)NCC1CCOCC1)CC)N   \n",
              "1509        1509          Clc1cc2nc(n(c2cc1)C(CC(=O)NCc1ncccc1)CC)N   \n",
              "1510        1510             Brc1cc(ccc1)C1CC1C=1N=C(N)N(C)C(=O)C=1   \n",
              "1511        1511       O=C1N(C)C(=NC(=C1)C1CC1c1cc(ccc1)-c1ccccc1)N   \n",
              "1512        1512                Clc1cc2nc(n(c2cc1)CCCC(=O)NCC1CC1)N   \n",
              "\n",
              "            CID  Class  Model     pIC50  \n",
              "0        BACE_1      1  Train  9.154901  \n",
              "1        BACE_2      1  Train  8.853872  \n",
              "2        BACE_3      1  Train  8.698970  \n",
              "3        BACE_4      1  Train  8.698970  \n",
              "4        BACE_5      1  Train  8.698970  \n",
              "...         ...    ...    ...       ...  \n",
              "1508  BACE_1543      0   Test  3.000000  \n",
              "1509  BACE_1544      0   Test  3.000000  \n",
              "1510  BACE_1545      0   Test  2.953115  \n",
              "1511  BACE_1546      0   Test  2.733298  \n",
              "1512  BACE_1547      0   Test  2.544546  \n",
              "\n",
              "[1513 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f38a47d-99de-4a1e-893b-e3b16cacedac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>mol</th>\n",
              "      <th>CID</th>\n",
              "      <th>Class</th>\n",
              "      <th>Model</th>\n",
              "      <th>pIC50</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>O1CC[C@@H](NC(=O)[C@@H](Cc2cc3cc(ccc3nc2N)-c2c...</td>\n",
              "      <td>BACE_1</td>\n",
              "      <td>1</td>\n",
              "      <td>Train</td>\n",
              "      <td>9.154901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Fc1cc(cc(F)c1)C[C@H](NC(=O)[C@@H](N1CC[C@](NC(...</td>\n",
              "      <td>BACE_2</td>\n",
              "      <td>1</td>\n",
              "      <td>Train</td>\n",
              "      <td>8.853872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...</td>\n",
              "      <td>BACE_3</td>\n",
              "      <td>1</td>\n",
              "      <td>Train</td>\n",
              "      <td>8.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>S1(=O)(=O)C[C@@H](Cc2cc(O[C@H](COCC)C(F)(F)F)c...</td>\n",
              "      <td>BACE_4</td>\n",
              "      <td>1</td>\n",
              "      <td>Train</td>\n",
              "      <td>8.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>S1(=O)(=O)N(c2cc(cc3c2n(cc3CC)CC1)C(=O)N[C@H](...</td>\n",
              "      <td>BACE_5</td>\n",
              "      <td>1</td>\n",
              "      <td>Train</td>\n",
              "      <td>8.698970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1508</th>\n",
              "      <td>1508</td>\n",
              "      <td>Clc1cc2nc(n(c2cc1)C(CC(=O)NCC1CCOCC1)CC)N</td>\n",
              "      <td>BACE_1543</td>\n",
              "      <td>0</td>\n",
              "      <td>Test</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1509</th>\n",
              "      <td>1509</td>\n",
              "      <td>Clc1cc2nc(n(c2cc1)C(CC(=O)NCc1ncccc1)CC)N</td>\n",
              "      <td>BACE_1544</td>\n",
              "      <td>0</td>\n",
              "      <td>Test</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1510</th>\n",
              "      <td>1510</td>\n",
              "      <td>Brc1cc(ccc1)C1CC1C=1N=C(N)N(C)C(=O)C=1</td>\n",
              "      <td>BACE_1545</td>\n",
              "      <td>0</td>\n",
              "      <td>Test</td>\n",
              "      <td>2.953115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>1511</td>\n",
              "      <td>O=C1N(C)C(=NC(=C1)C1CC1c1cc(ccc1)-c1ccccc1)N</td>\n",
              "      <td>BACE_1546</td>\n",
              "      <td>0</td>\n",
              "      <td>Test</td>\n",
              "      <td>2.733298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1512</th>\n",
              "      <td>1512</td>\n",
              "      <td>Clc1cc2nc(n(c2cc1)CCCC(=O)NCC1CC1)N</td>\n",
              "      <td>BACE_1547</td>\n",
              "      <td>0</td>\n",
              "      <td>Test</td>\n",
              "      <td>2.544546</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1513 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f38a47d-99de-4a1e-893b-e3b16cacedac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f38a47d-99de-4a1e-893b-e3b16cacedac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f38a47d-99de-4a1e-893b-e3b16cacedac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-80c2c68c-af3d-4478-abc9-71409bd1d777\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80c2c68c-af3d-4478-abc9-71409bd1d777')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-80c2c68c-af3d-4478-abc9-71409bd1d777 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_89bd73bf-db3a-4b9f-8804-ff2851241ff7\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('bace_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_89bd73bf-db3a-4b9f-8804-ff2851241ff7 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('bace_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "bace_df",
              "summary": "{\n  \"name\": \"bace_df\",\n  \"rows\": 1513,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 436,\n        \"min\": 0,\n        \"max\": 1512,\n        \"num_unique_values\": 1513,\n        \"samples\": [\n          1001,\n          618,\n          806\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mol\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1513,\n        \"samples\": [\n          \"s1nc(cc1)-c1cc2c(OC(CC23N=C(N)N(C)C3=O)(C)C)cc1\",\n          \"FC(F)Oc1ccc(cc1)[C@@]1(N=C(N)N(C)C1=O)c1cc(ccc1)\\\\C=C\\\\CCCO\",\n          \"S1(=O)(=O)N(CCCC1)c1cc(cc(NCC)c1)C(=O)NC(Cc1ccccc1)C(O)C[NH2+]C1CC1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1513,\n        \"samples\": [\n          \"BACE_1034\",\n          \"BACE_649\",\n          \"BACE_839\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Train\",\n          \"Valid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pIC50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.342416931106123,\n        \"min\": 2.5445461,\n        \"max\": 10.522879,\n        \"num_unique_values\": 651,\n        \"samples\": [\n          3.4689045,\n          7.167491\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data_setup.py"
      ],
      "metadata": {
        "id": "tigmQnnKHr7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "import_start_time = timer()\n",
        "import subprocess\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import InMemoryDataset, Dataset, Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "graphdata_start_time = timer()\n",
        "from deepchem.feat.graph_data import GraphData\n",
        "graphdata_end_time = timer()\n",
        "graphdata_time = end_time-start_time\n",
        "import os\n",
        "import pandas as pd\n",
        "import os.path as osp\n",
        "import_end_time = timer()\n",
        "\n",
        "data_setup_module_imports_time = import_end_time- import_start_time\n",
        "\n",
        "# Custom torch geometric Dataset class to store the samples and their corresponding labels\n",
        "\n",
        "class MoleculeDataset(Dataset):\n",
        "  def __init__(self, root, csv_file, transform=None, pre_transform=None, pre_filter=None):\n",
        "    \"\"\"\n",
        "    Constructor method of the class\n",
        "\n",
        "    :root = Path where the dataset should be stored. This folder is split\n",
        "    into raw_dir (downloaded dataset) and processed_dir(processed data).\n",
        "    :csv_file = Desired name of the CSV file to be saved.\n",
        "    : transform, pre_transform, pre_filter = optional transforms\n",
        "    \"\"\"\n",
        "    self.csv_file = csv_file\n",
        "    super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    \"\"\"\n",
        "    If this file exists in raw_dir, the download is not triggered/\n",
        "    (the download function is not implemented here)\n",
        "    \"\"\"\n",
        "    return self.csv_file\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    \"\"\"\n",
        "    If these files are found in raw_dir, processing is skipped\n",
        "    \"\"\"\n",
        "    self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "\n",
        "    return [f'data_{i}.pt' for i in list(self.data.index)]\n",
        "\n",
        "  def download(self):\n",
        "    \"\"\"\n",
        "    No need to download the csv file as it is already downloaded\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def process(self):\n",
        "    \"\"\"\n",
        "    Converts molecules with SMILES formats into PyTorch graphs. Uses Deepchem's MolGraphConvFeaturizer to create a graph\n",
        "    and then convert that to a torch graph with to_pyg_graph. Saves these in the processed directory.\n",
        "    \"\"\"\n",
        "    self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "    featurizer=dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
        "\n",
        "    for idx, row in self.data.iterrows():\n",
        "      # Featurize molecule and convert to torch graph\n",
        "      smiles = row['mol']\n",
        "      label = row['Class']\n",
        "      pic50 = row['pIC50']\n",
        "\n",
        "      out = featurizer.featurize(smiles)\n",
        "      pyg_out = GraphData.to_pyg_graph(out[0])\n",
        "      pyg_out.Class = torch.tensor([label])\n",
        "      pyg_out.smiles = smiles\n",
        "      pyg_out.pic50 = pic50\n",
        "\n",
        "      # data = Data(x=pyg_out.x, edge_index=pyg_out.edge_index, edge_attr=pyg_out.edge_attr,\n",
        "      #            y=torch.tensor([label]), dtype = torch.float)\n",
        "\n",
        "      torch.save(pyg_out, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "\n",
        "  def len(self):\n",
        "    \"\"\"\n",
        "    Returns number of samples in the dataset\n",
        "    \"\"\"\n",
        "    return len(self.processed_file_names)\n",
        "\n",
        "  def get(self, idx):\n",
        "    \"\"\"\n",
        "    Loads a single graph\n",
        "    \"\"\"\n",
        "    data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "    return data\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(root_directory: str,\n",
        "                       batch_size: int,\n",
        "                       num_workers: int=NUM_WORKERS,\n",
        "                       train_fraction: float=0.8):\n",
        "  dataset = MoleculeDataset(root = root_directory, csv_file = \"bace.csv\").shuffle()\n",
        "\n",
        "  train_indices = int(train_fraction*len(dataset))\n",
        "\n",
        "  train_dataset = dataset[:train_indices]\n",
        "  test_dataset = dataset[train_indices:]\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "  return dataset, train_dataloader, test_dataloader\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bValmSU_LkC",
        "outputId": "7d222f0c-b7f6-4422-a5ad-bc3222b7db3d"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models.py"
      ],
      "metadata": {
        "id": "B7UYYjyETSoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/models.py\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "import subprocess\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, GraphConv, GATConv, MLP, GINConv, global_max_pool, SAGPooling, TopKPooling, GINEConv\n",
        "from torch.nn import Linear, ReLU, Dropout, Softmax\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#----------------------------------------------------------------------GCNClassifier----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class GCNClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Standard GCN graph classifier. Uses the graph convolutional operator from PyTorch geometric.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int, pool_method:torch_geometric.nn.pool):\n",
        "    \"\"\"\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Convolutional Layers\n",
        "    self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "\n",
        "    self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    # Linear layer used in classification\n",
        "    self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    # Pooling method\n",
        "    self.pool_method = pool_method\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    Returns probabilities of the two classes (drug/not drug)\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtain node embeddings\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.conv3(x, edge_index)\n",
        "\n",
        "    # Aggregate node embeddings\n",
        "    x = self.pool_method(x, batch)\n",
        "\n",
        "    # Regularisation\n",
        "    x = F.dropout(x)\n",
        "\n",
        "    # Classification\n",
        "    x = self.lin(x)\n",
        "\n",
        "    x = F.softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "class GraphConvClassifier(GCNClassifier):\n",
        "  \"\"\"\n",
        "  Same architecture as GCN Classifier however uses GraphConv layers\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int,  pool_method:torch_geometric.nn.pool):\n",
        "    super().__init__(in_channels, hidden_channels, out_channels, pool_method)\n",
        "    self.conv1 = GraphConv(in_channels, hidden_channels)\n",
        "\n",
        "    self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    self.pool_method = pool_method\n",
        "\n",
        "#------------------------------------------------------------------GATCLassifier------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class GATClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  GAT Convolutional graph classifier. Uses the graph attention operator from PyTorch geometric\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int, heads:int, pool_method:torch_geometric.nn.pool,\n",
        "               use_edge_attr:bool):\n",
        "    \"\"\"\n",
        "    :in_channels = number of features of the graph's nodes.\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network.\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : heads = the number of multi-headed attentions.\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    : use_edge_attr = boolean variable which determines whether to use the edge attributes of the graph.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Convolutional Layers\n",
        "    self.conv1 = GATConv(in_channels,\n",
        "                         hidden_channels,\n",
        "                         heads,\n",
        "                         concat = True)\n",
        "    self.conv2 = GATConv(hidden_channels*heads,\n",
        "                         hidden_channels,\n",
        "                         heads,\n",
        "                         concat=True)\n",
        "    self.conv3 = GATConv(hidden_channels*heads,\n",
        "                         hidden_channels,\n",
        "                         1,\n",
        "                         concat=False)\n",
        "    self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    # Pooling method\n",
        "    self.pool_method = pool_method\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    Returns probabilities of the two classes (drug/not drug)\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtain node embeddings\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "\n",
        "    # can use edge attributes\n",
        "    if use_edge_attr:\n",
        "      x = self.conv1(x, edge_index, edge_attr)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv2(x, edge_index, edge_attr)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv3(x, edge_index, edge_attr)\n",
        "\n",
        "    # not using edge attributes\n",
        "    else:\n",
        "      x = self.conv1(x, edge_index)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv2(x, edge_index)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv3(x, edge_index)\n",
        "\n",
        "    # Aggregate node embeddings\n",
        "    x = self.pool_method(x, batch)\n",
        "\n",
        "    # Regularisation\n",
        "    x = F.dropout(x)\n",
        "\n",
        "    # Classification\n",
        "    x = self.lin(x)\n",
        "\n",
        "    x = F.softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "#-------------------------------------------------------------------GINConvClassifier-------------------------------------------------------------------\n",
        "\n",
        "class GINConvClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Applies the graph isomorphism operator\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, pool_method: torch_geometric.nn.pool):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : num_layers = the number of layers of the multi-layer perceptron\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.conv = GINConv\n",
        "    self.pool_method = pool_method\n",
        "\n",
        "    # Create multiple GINConv layers as specified by num_layers\n",
        "    for _ in range(num_layers):\n",
        "      mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "      self.convs.append(self.conv(nn=mlp, train_eps=False))\n",
        "      in_channels = hidden_channels\n",
        "\n",
        "    # Define the final MLP\n",
        "    self.mlp = MLP([hidden_channels, hidden_channels, out_channels], norm = None, dropout = 0.5)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    Returns probabilities of the two classes (drug/not drug)\n",
        "    \"\"\"\n",
        "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "    for conv in self.convs:\n",
        "      x = conv(x, edge_index).relu()\n",
        "    x = self.pool_method(x, batch)\n",
        "    return self.mlp(x)\n",
        "\n",
        "#----------------------------------------------------------------------GINEConvClassifier----------------------------------------------------------------------\n",
        "\n",
        "class GINEConvClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Same as the GINConvClassifier, however also uses edge attributes of the graphs\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, pool_method: torch_geometric.nn.pool,\n",
        "               use_edge_attr:bool, edge_dim:int):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : num_layers = the number of layers of the multi-layer perceptron\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    : use_edge_attr = boolean variable to determine whether will use the edge attributes or not.\n",
        "    : edge_dim = the dimensionality of the edge attributes for the graph's edges\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.conv = GINEConv\n",
        "    self.pool_method = pool_method\n",
        "    self.use_edge_attr = use_edge_attr\n",
        "    self.edge_dim = edge_dim\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "      self.convs.append(self.conv(nn=mlp, train_eps=False, edge_dim=self.edge_dim))\n",
        "      in_channels = hidden_channels\n",
        "\n",
        "    self.mlp = MLP([hidden_channels, hidden_channels, out_channels], norm = None, dropout = 0.5)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    Returns probabilities of the two classes (drug/not drug)\n",
        "    \"\"\"\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "    for conv in self.convs:\n",
        "      if self.use_edge_attr:\n",
        "        x = conv(x, edge_index, edge_attr).relu()\n",
        "      else:\n",
        "        x = conv(x, edge_index).relu()\n",
        "\n",
        "    x = self.pool_method(x, batch)\n",
        "    return self.mlp(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "YZHgZtyPKEx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c189f71f-737c-4ca1-ae93-84360d62c2da"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# engine.py"
      ],
      "metadata": {
        "id": "g4Ytc7sMiqBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from pathlib import Path\n",
        "from typing import Callable, Optional, Any\n",
        "import pickle\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from tqdm.auto import tqdm\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------train_step--------------------------------------------------------------------\n",
        "\n",
        "def train_step(model:torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device):\n",
        "  \"\"\"\n",
        "  Performs the training of a model for one epoch for the training dataloader.\n",
        "\n",
        "  Returns lists of the training loss, accuracy and AUC of the training dataloader for the epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  train_loss, train_acc, train_auc = 0, 0, 0\n",
        "\n",
        "  # We time how long it takes for each section in the training process\n",
        "  auc_time = 0\n",
        "  out_time = 0\n",
        "  loss_time = 0\n",
        "  optimizer_time = 0\n",
        "  section_time = 0\n",
        "  dataloader_loop_time = 0\n",
        "  inside_loop_time = 0\n",
        "\n",
        "\n",
        "  loop_start_time = timer()\n",
        "\n",
        "  # Loop over the batches\n",
        "  for idx, batch in enumerate(dataloader):\n",
        "    # print(f\"entered {idx} loop of train step\")\n",
        "    inside_loop_start_time = timer()\n",
        "    # Time how long it takes to obtain an idx and batch of the dataloader\n",
        "    if idx > 1:\n",
        "      dataloader_loop_end_time = timer()\n",
        "      dataloader_loop_time += dataloader_loop_end_time-dataloader_loop_start_time\n",
        "\n",
        "    # Can time how long any chosen section takes to run\n",
        "    section_start_time = timer()\n",
        "    to_device_start_time = timer()\n",
        "    batch = batch.to(device)\n",
        "    to_device_end_time = timer()\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out_start_time = timer()\n",
        "    out = model(batch)\n",
        "    out_end_time = timer()\n",
        "    out_time+=out_end_time-out_start_time\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(out, batch.Class)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Calculate the label predictions\n",
        "    label_preds = torch.argmax(out, dim=1)\n",
        "    # Calculate accuracy\n",
        "    train_acc += (label_preds == batch.Class).sum()\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_start_time = timer()\n",
        "    # Check both classes present in batch.Class, otherwise add the batch_auc from the previous iteration\n",
        "    if len(torch.unique(batch.Class)) == 2:\n",
        "        batch_auc = roc_auc_score(batch.Class.detach().cpu().numpy(), out[:,1].detach().cpu().numpy())\n",
        "        train_auc += batch_auc\n",
        "    else:\n",
        "      train_auc += batch_auc\n",
        "\n",
        "    auc_end_time = timer()\n",
        "    auc_time += auc_end_time-auc_start_time\n",
        "\n",
        "\n",
        "    # Loss backward\n",
        "    loss_start_time = timer()\n",
        "    loss.backward()\n",
        "    loss_end_time = timer()\n",
        "    loss_time += loss_end_time-loss_start_time\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer_start_time = timer()\n",
        "    optimizer.step()\n",
        "    optimizer_end_time = timer()\n",
        "    optimizer_time = optimizer_end_time-optimizer_start_time\n",
        "    section_end_time = timer()\n",
        "    section_time+=section_end_time-section_start_time\n",
        "    dataloader_loop_start_time = timer()\n",
        "    inside_loop_end_time = timer()\n",
        "    inside_loop_time += inside_loop_end_time-inside_loop_start_time\n",
        "\n",
        "\n",
        "  loop_end_time = timer()\n",
        "  # print(f\"Section time is {section_time:.4f}\")\n",
        "  # print(f\"Dataloader loop time is {dataloader_loop_time:.4f}\")\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss/len(dataloader.dataset)\n",
        "  train_acc = train_acc/len(dataloader.dataset)\n",
        "  train_auc = train_auc/len(dataloader)\n",
        "  # print(f\"AUC calculation time: {auc_time:.4f}s, Forward pass: {out_time:.4f}s, Loss time: {loss_time:.4f}, Optimizer time: {optimizer_time:.4f}, To device time: {to_device_end_time-to_device_start_time:.4f}\\n\")\n",
        "\n",
        "\n",
        "  #print(f\"Train outside loop time is {loop_end_time-loop_start_time:.4f}, inside loop time is {inside_loop_time:.4f}\")\n",
        "\n",
        "  return train_loss, train_acc, train_auc\n",
        "\n",
        "#--------------------------------------------------------------------test_step------------------------------------------------------------------------\n",
        "\n",
        "def test_step(model:torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              device: torch.device):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs the testing of a model for one epoch for the test dataloader.\n",
        "\n",
        "  Returns lists of the test loss, accuracy and AUC of the test dataloader for the epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc, test_auc = 0, 0, 0\n",
        "\n",
        "  # Turn on torch inference manager\n",
        "  with torch.inference_mode():\n",
        "    # Loop through data batches\n",
        "    for idx, batch in enumerate(dataloader):\n",
        "      # print(f\"entered test step {idx} batch loop\")\n",
        "      batch = batch.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      out = model(batch)\n",
        "\n",
        "      # Calculate the loss\n",
        "      loss = loss_fn(out, batch.Class)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate the label predictions\n",
        "      label_preds = torch.argmax(out, dim=1)\n",
        "      # Calculate accuracy\n",
        "      test_acc += (label_preds == batch.Class).sum()/len(label_preds)\n",
        "\n",
        "      # Calculate the AUC\n",
        "      if len(torch.unique(batch.Class)) == 2:\n",
        "        batch_auc = roc_auc_score(batch.Class.detach().cpu().numpy(), out[:,1].detach().cpu().numpy())\n",
        "        test_auc += batch_auc\n",
        "      else:\n",
        "        test_auc += batch_auc\n",
        "\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss/len(dataloader)\n",
        "    test_acc = test_acc/len(dataloader)\n",
        "    test_auc = test_auc/len(dataloader)\n",
        "\n",
        "    return test_loss, test_acc, test_auc\n",
        "\n",
        "#--------------------------------------------------------------------moving_average--------------------------------------------------------------------\n",
        "\n",
        "def moving_average(values:list , window_size:int):\n",
        "    \"\"\"\n",
        "    Calculates the simple moving average of the last window_size elements in a list of values\n",
        "\n",
        "    Returns the average\n",
        "    \"\"\"\n",
        "    if len(values) < window_size:\n",
        "        return None\n",
        "    return sum(values[-window_size:]) / window_size\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------train-----------------------------------------------------------------------------\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          device: torch.device,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n",
        "          epochs: int =5,\n",
        "          model_save_path: str = None,\n",
        "          window_size: int=10,\n",
        "          patience: int=10):\n",
        "  \"\"\"\n",
        "  Combines train_step and test_step to train a model and evaluate it at each epoch on the train_dataloader and test_dataloader.\n",
        "\n",
        "  :model_save_path = a string used to save the model's parameters. If no string is provided it is not saved.\n",
        "\n",
        "  Returns a dictionary of results.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # 1. Create empty results dictionary\n",
        "  results = {\"epoch\": [],\n",
        "             \"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"train_auc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": [],\n",
        "             \"test_auc\": [],\n",
        "             \"saved_epochs\": [],\n",
        "             \"test_loss_mov_avg\": [],\n",
        "             \"test_auc_mov_avg\": []}\n",
        "  # 2. Loop through training and testing steps for a number of epochs\n",
        "  best_moving_loss_avg = float('inf')\n",
        "  best_moving_auc_avg = 0\n",
        "\n",
        "  # Loop over the number of epochs\n",
        "  for i in tqdm(range(epochs)):\n",
        "\n",
        "    start_time = timer()\n",
        "    train_step_start_time = timer()\n",
        "    train_loss, train_acc, train_auc = train_step(model,\n",
        "                                       train_dataloader,\n",
        "                                       loss_fn,\n",
        "                                       optimizer,\n",
        "                                       device)\n",
        "    train_step_end_time = timer()\n",
        "    test_step_start_time = timer()\n",
        "    test_loss, test_acc, test_auc = test_step(model,\n",
        "                                    test_dataloader,\n",
        "                                    loss_fn,\n",
        "                                    optimizer,\n",
        "                                    device)\n",
        "    test_step_end_time = timer()\n",
        "    # print(f\"Train step time is {train_step_end_time-train_step_start_time:.4f}s, Test step time is {test_step_end_time-test_step_start_time:.4f}s\\n\")\n",
        "\n",
        "    # 3. Print out what's happening\n",
        "    print(f\"Epoch: {i}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Train AUC: {train_auc:.4f}, Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}, Test auc: {test_auc:.4f}\")\n",
        "    # 4. Update results dictionary\n",
        "    append_start_time = timer()\n",
        "    results[\"epoch\"].append(i)\n",
        "    results[\"train_loss\"].append(round(train_loss, 4))\n",
        "    results[\"train_acc\"].append(round(train_acc.item(), 4))\n",
        "    results[\"train_auc\"].append(round(train_auc, 4))\n",
        "    results[\"test_loss\"].append(round(test_loss, 4))\n",
        "    results[\"test_acc\"].append(round(test_acc.item(), 4))\n",
        "    results[\"test_auc\"].append(round(test_auc, 4))\n",
        "    append_end_time = timer()\n",
        "    # print(f\"append time is{append_end_time-append_start_time:.4f}\")\n",
        "\n",
        "    # 5. If model_save_path provided, save the model to its path based on whether test loss and test AUC have improved.\n",
        "    \"\"\"\n",
        "    Once the number of epochs is greater than the window_size a current moving average is created of the last 'window_size'\n",
        "    loss and AUC values. As long as these current metrics are higher than the current moving average, the model is saved.\n",
        "    If the current metrics are not better than the current moving average for 'patience' epochs the training stops early.\n",
        "    \"\"\"\n",
        "\n",
        "    save_timer_start = timer()\n",
        "    if model_save_path:\n",
        "      current_moving_loss_avg = moving_average(results[\"test_loss\"], window_size)\n",
        "      if current_moving_loss_avg is not None:\n",
        "        results[\"test_loss_mov_avg\"].append(round(current_moving_loss_avg, 4))\n",
        "      else:\n",
        "        results[\"test_loss_mov_avg\"].append(None)\n",
        "\n",
        "\n",
        "      current_moving_auc_avg = moving_average(results[\"test_auc\"], window_size)\n",
        "      if current_moving_auc_avg is not None:\n",
        "        results[\"test_auc_mov_avg\"].append(round(current_moving_auc_avg, 4))\n",
        "      else:\n",
        "        results[\"test_auc_mov_avg\"].append(None)\n",
        "\n",
        "      if current_moving_loss_avg is not None and current_moving_auc_avg is not None:\n",
        "        if current_moving_loss_avg < best_moving_loss_avg and current_moving_auc_avg > best_moving_auc_avg:\n",
        "          without_improvement_count = 0\n",
        "          results[\"saved_epochs\"].append(i)\n",
        "          torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
        "                   f=model_save_path)\n",
        "          print(f\"Saved model at epoch {i} with current average test loss: {current_moving_loss_avg:.4f} and previous best: {best_moving_loss_avg:.4f}\")\n",
        "          print(f\"Saved model at epoch {i} with current average AUC loss: {current_moving_auc_avg:.4f} and previous best: {best_moving_auc_avg:.4f}\")\n",
        "          best_moving_loss_avg = current_moving_loss_avg\n",
        "          best_moving_auc_avg = current_moving_auc_avg\n",
        "\n",
        "        else:\n",
        "          without_improvement_count += 1\n",
        "          print(f\"Without_improvement_count: {without_improvement_count}\")\n",
        "        if without_improvement_count > patience:\n",
        "          print(\"Early Stopping\")\n",
        "          break\n",
        "    save_timer_end = timer()\n",
        "    end_time  = timer()\n",
        "    print(f\"Epoch took {end_time-start_time:.2f} seconds\")\n",
        "    # print(f\"Time to save loop : {save_timer_end-save_timer_start:.4f}\")\n",
        "\n",
        "  # 6. Return the filled results at the end of the epochs\n",
        "\n",
        "  return results\n",
        "\n",
        "#---------------------------------------------------------------------run_model_repeats---------------------------------------------------------------------\n",
        "\n",
        "def run_model_repeats(model: torch.nn.Module,\n",
        "                      device: torch.device,\n",
        "                      train_dataloader: torch.utils.data.DataLoader,\n",
        "                      test_dataloader: torch.utils.data.DataLoader,\n",
        "                      optimizer_: Callable[[], torch.optim.Optimizer],\n",
        "                      criterion: torch.nn.Module,\n",
        "                      models_directory: Path=None,\n",
        "                      num_hidden_channels: int = 128,\n",
        "                      pool_method: Any = global_mean_pool,\n",
        "                      nb_epochs: int = 300,\n",
        "                      nb_repeats: int = 1,\n",
        "                      window_size: int = 10,\n",
        "                      patience: int = 50):\n",
        "  \"\"\"\n",
        "  Runs training runs for 'nb_repeats' and optionally saves them if nb_repeats.\n",
        "  Optionally save the model and its results if models_directory provided.\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(nb_repeats):\n",
        "    if models_directory:\n",
        "      model_save_name = f\"{i}_{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}.pth\"\n",
        "      model_save_path  = models_directory / model_save_name\n",
        "    else:\n",
        "      model_save_path = None\n",
        "    optimizer = optimizer_(model.parameters())\n",
        "\n",
        "\n",
        "    results = train(model,\n",
        "        device,\n",
        "        train_dataloader,\n",
        "        test_dataloader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        epochs = nb_epochs,\n",
        "        model_save_path = model_save_path,\n",
        "        window_size=window_size,\n",
        "        patience=patience)\n",
        "    if models_directory:\n",
        "      with open(models_directory/f\"{i}_{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_results.pkl\", 'wb') as f:\n",
        "        print(\"Saved results of this model\")\n",
        "        pickle.dump(results, f)\n",
        "    else:\n",
        "      print(\"Did not save results of this model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-RZqXGtUKE0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0142409a-2bfc-4344-fa5e-834de0e9a629"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils.py"
      ],
      "metadata": {
        "id": "hwhs3Vu4iyZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from pathlib import Path\n",
        "\n",
        "#------------------------------------------------------------adam_optimizer_callable------------------------------------------------------------\n",
        "\n",
        "\n",
        "def adam_optimizer_callable(parameters, lr=0.001, weight_decay=0):\n",
        "    return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "#-----------------------------------------------------------------new_metric_func-----------------------------------------------------------------\n",
        "\n",
        "\n",
        "def new_metric_func(model, train_dataset, test_dataset, train_dataloader, test_dataloader):\n",
        "  \"\"\"\n",
        "  Calculates the metrics (AUC and Scikit-Learn's Classification report metrics) for a model for its training data and test data.\n",
        "\n",
        "  Returns training and test AUROC curve and training and test Sklearn classification report as a dataframe\n",
        "  \"\"\"\n",
        "  with torch.inference_mode():\n",
        "    model.eval()\n",
        "\n",
        "    # Create empty tensors to fill with probabilities, predictions and labels\n",
        "    total_train_probs = torch.empty(len(train_dataset))\n",
        "    total_train_preds = torch.empty(len(train_dataset))\n",
        "    total_train_labels = torch.empty(len(train_dataset))\n",
        "\n",
        "    total_test_probs = torch.empty(len(test_dataset))\n",
        "    total_test_preds = torch.empty(len(test_dataset))\n",
        "    total_test_labels = torch.empty(len(test_dataset))\n",
        "\n",
        "    # Loop over batches and add to the total tensors\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "      batch = batch.to(device)\n",
        "      batch_size = 32\n",
        "      current_batch_size = len(batch)\n",
        "      # print(f\"{idx*batch_size}:{idx*batch_size+current_batch_size}\")\n",
        "      out = model.forward(batch)\n",
        "      train_probs = out[:,1]\n",
        "      train_preds = torch.argmax(out, dim=1)\n",
        "\n",
        "      total_train_probs[idx*batch_size:idx*batch_size+current_batch_size] = train_probs\n",
        "      total_train_preds[idx*batch_size:idx*batch_size+current_batch_size] = train_preds\n",
        "      total_train_labels[idx*batch_size:idx*batch_size+current_batch_size] = batch.Class\n",
        "\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "      batch = batch.to(device)\n",
        "      batch_size = 32\n",
        "      current_batch_size = len(batch)\n",
        "      # print(f\"{idx*batch_size}:{idx*batch_size+current_batch_size}\")\n",
        "      out = model.forward(batch)\n",
        "      test_probs = out[:,1]\n",
        "      test_preds = torch.argmax(out, dim=1)\n",
        "\n",
        "      total_test_probs[idx*batch_size:idx*batch_size+current_batch_size] = test_probs\n",
        "      total_test_preds[idx*batch_size:idx*batch_size+current_batch_size] = test_preds\n",
        "      total_test_labels[idx*batch_size:idx*batch_size+current_batch_size] = batch.Class\n",
        "\n",
        "    # Calculate AUC and dataframes of metrics (using Scikit-Learn's metrics)\n",
        "    train_auroc = roc_auc_score(total_train_labels, total_train_probs).item()\n",
        "    train_classification_report = classification_report(total_train_labels, total_train_preds, output_dict=True)\n",
        "    train_report_df = pd.DataFrame(data=train_classification_report).transpose()\n",
        "\n",
        "    test_auroc = roc_auc_score(total_test_labels, total_test_probs).item()\n",
        "    test_classification_report = classification_report(total_test_labels, total_test_preds, output_dict=True)\n",
        "    test_report_df = pd.DataFrame(data=test_classification_report).transpose()\n",
        "\n",
        "\n",
        "  return train_auroc, test_auroc, train_report_df, test_report_df\n",
        "\n",
        "#---------------------------------------------------------------average_model_metrics---------------------------------------------------------------\n",
        "\n",
        "def average_model_metrics(model: torch.nn.Module,\n",
        "                          models_directory: Path,\n",
        "                          model_name_stem: str,\n",
        "                          repeats: int,\n",
        "                          save_yes_no: bool):\n",
        "  \"\"\"\n",
        "  Takes a GNN model architecture and a directory where its weights are stored (repeats) then return an average of their performance metrics.\n",
        "  Uses new_metric_func\n",
        "\n",
        "  :model = instance of a classification model which will load the model parameters of trained models\n",
        "  :model_directory = directory where the model parameters are stored\n",
        "  : model_name_stem = name stem of the repeats e.g. \"_128_300_global_mean_pool.pth\"\n",
        "  : repeats = number of repeats in the directory\n",
        "  : save_yes_no = option to save the results as pickle files in the same directory\n",
        "\n",
        "  Returns dataframes of the mean and stadndard deviation of the training and test performance metrics (AUROC, Sklearn metrics).\n",
        "  \"\"\"\n",
        "  # Will append results of each model's performance to these lists (AUROC and classification report metric)\n",
        "  train_auroc_list = []\n",
        "  test_auroc_list = []\n",
        "  train_report_list = []\n",
        "  test_report_list = []\n",
        "\n",
        "  # Loop over the number of repeats of the model runs\n",
        "  for i in range(repeats):\n",
        "    model_name = f\"{i}\" + model_name_stem\n",
        "    model_path = models_directory / model_name\n",
        "    model.load_state_dict(torch.load(f=model_path))\n",
        "    model.to(device)\n",
        "    train_auroc, test_auroc, train_classification_report, test_classification_report = new_metric_func(model, train_dataloader, test_dataloader)\n",
        "    train_auroc_list.append(train_auroc)\n",
        "    test_auroc_list.append(test_auroc)\n",
        "    train_report_list.append(pd.DataFrame(train_classification_report))\n",
        "    test_report_list.append(pd.DataFrame(test_classification_report))\n",
        "\n",
        "  # Calculate averages and standard deviations of our repeats\n",
        "  mean_train_auroc = np.mean(train_auroc_list)\n",
        "  mean_test_auroc = np.mean(test_auroc_list)\n",
        "\n",
        "  std_train_auroc = np.std(train_auroc_list)\n",
        "  std_test_auroc = np.std(test_auroc_list)\n",
        "\n",
        "  auroc_data = {\"Train\": [mean_train_auroc, std_train_auroc],\n",
        "               \"Test\": [mean_test_auroc, std_test_auroc]}\n",
        "  auroc_df = pd.DataFrame(auroc_data)\n",
        "\n",
        "\n",
        "  mean_train_model_metrics = pd.DataFrame(pd.concat(train_report_list).groupby(level=0).mean())\n",
        "  mean_test_model_metrics = pd.DataFrame(pd.concat(test_report_list).groupby(level=0).mean())\n",
        "\n",
        "  std_train_model_metrics = pd.DataFrame(pd.concat(train_report_list).groupby(level=0).std())\n",
        "  std_test_model_metrics = pd.DataFrame(pd.concat(test_report_list).groupby(level=0).std())\n",
        "\n",
        "  # Option to save the results as pickle files (which can later be loaded and turned into dataframes)\n",
        "  if save_yes_no:\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_auroc_df.pkl\", 'wb') as f:\n",
        "      print(\"Saved auroc dataframe\")\n",
        "      pickle.dump(auroc_df, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_mean_train_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved mean train metrics\")\n",
        "      pickle.dump(mean_train_model_metrics, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_mean_test_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved mean test metrics\")\n",
        "      pickle.dump(mean_test_model_metrics, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_std_train_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved std train metrics\")\n",
        "      pickle.dump(std_train_model_metrics, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_std_test_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved std test metrics\")\n",
        "      pickle.dump(std_test_model_metrics, f)\n",
        "\n",
        "\n",
        "  return auroc_df, mean_train_model_metrics, mean_test_model_metrics, std_train_model_metrics, std_test_model_metrics\n",
        "\n",
        "#---------------------------------------------------------------loss_acc_auc_plots---------------------------------------------------------------\n",
        "\n",
        "\n",
        "def loss_acc_auc_plots(results:dict):\n",
        "  \"\"\"\n",
        "  Plots the loss, accuracy metric and AUCROC metric curves of the training and test set of a model results (dictionary).\n",
        "\n",
        "  Returns three plots.\n",
        "  \"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots(ncols=3, nrows=1, figsize = (15,6))\n",
        "\n",
        "  ax[0].plot(results[\"epoch\"], results[\"train_loss\"], label=\"Train\");\n",
        "  ax[0].plot(results[\"epoch\"], results[\"test_loss\"],  label=\"Test\");\n",
        "  ax[0].vlines(results[\"saved_epochs\"], ymin=0, ymax = max(results[\"test_loss\"]), alpha=0.2, colors='black', linestyle='dashed', label = 'Saved epochs')\n",
        "\n",
        "  ax[1].plot(results[\"epoch\"], results[\"train_acc\"], label=\"Train\");\n",
        "  ax[1].plot(results[\"epoch\"], results[\"test_acc\"],  label=\"Test\");\n",
        "  ax[1].vlines(results[\"saved_epochs\"], ymin=0, ymax = max(results[\"train_acc\"]), alpha=0.2, colors='black', linestyle='dashed', label = 'Saved epochs')\n",
        "\n",
        "  ax[2].plot(results[\"epoch\"], results[\"train_auc\"], label=\"Train\");\n",
        "  ax[2].plot(results[\"epoch\"], results[\"test_auc\"],  label=\"Test\");\n",
        "  ax[2].vlines(results[\"saved_epochs\"], ymin=0, ymax = max(results[\"train_auc\"]), alpha=0.2, colors='black', linestyle='dashed', label = 'Saved epochs')\n",
        "\n",
        "  ax[0].set_xlabel(\"Epochs\", size=14)\n",
        "  ax[0].set_ylabel(\"Loss\", size=14)\n",
        "\n",
        "  ax[1].set_xlabel(\"Epochs\", size=14)\n",
        "  ax[1].set_ylabel(\"Accuracy\", size=14)\n",
        "\n",
        "  ax[2].set_xlabel(\"Epochs\", size=14)\n",
        "  ax[2].set_ylabel(\"AUC\", size=14)\n",
        "\n",
        "  ax[0].legend();\n",
        "  ax[1].legend();\n",
        "  ax[2].legend();"
      ],
      "metadata": {
        "id": "A_46kY4vKE22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca465af7-7299-4395-8889-976906bb2a09"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYvCPapHKE5O"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UsuPwgYKE7W"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.py"
      ],
      "metadata": {
        "id": "zWrFclFvHv_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "total_import_time_start = timer()\n",
        "import torch\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "start_time = timer()\n",
        "import data_setup\n",
        "from data_setup import *\n",
        "end_time = timer()\n",
        "print(f\"Imported data_setup.py, took {end_time - start_time:.4f}\")\n",
        "\n",
        "start_time = timer()\n",
        "import models\n",
        "from models import *\n",
        "end_time = timer()\n",
        "print(f\"Imported models.py, took {end_time - start_time:.4f}\")\n",
        "\n",
        "start_time = timer()\n",
        "import engine\n",
        "from engine import *\n",
        "end_time = timer()\n",
        "print(f\"Imported engine.py, took {end_time - start_time:.4f}\")\n",
        "\n",
        "start_time = timer()\n",
        "import utils\n",
        "from utils import *\n",
        "end_time = timer()\n",
        "print(f\"Imported utils.py, took {end_time-start_time:.4f}\")\n",
        "\n",
        "total_import_time_end = timer()\n",
        "\n",
        "start_time = timer()\n",
        "dataset, train_dataloader, test_dataloader = create_dataloaders(root_directory = \"going_modular/data/\",\n",
        "                                                                batch_size = 32)\n",
        "end_time = timer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(f\"time to import modules in data_setup.py: {data_setup_module_imports_time}\")\n",
        "print(f\"time to import graphdata in data_setup.py: {graphdata_time}\")\n",
        "print(f\"Total import time: {total_import_time_end - total_import_time_start}\")\n",
        "\n",
        "print(f\"Created dataset, train_dataloader, test_dataloader, took {end_time - start_time:.4f}s\")\n",
        "\n",
        "num_features = next(iter(train_dataloader)).x.shape[1]\n",
        "num_hidden_channels = 128\n",
        "num_out_channels = 2\n",
        "pool_method = global_mean_pool\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\"\"\"\n",
        "run_model_repeats(model = GCNClassifier(num_features, num_hidden_channels, num_out_channels, global_mean_pool),\n",
        "                  device = device,\n",
        "                  train_dataloader = train_dataloader,\n",
        "                  test_dataloader = test_dataloader,\n",
        "                  optimizer_ = adam_optimizer_callable,\n",
        "                  criterion = torch.nn.CrossEntropyLoss(),\n",
        "                  num_hidden_channels = 128,\n",
        "                  pool_method = global_mean_pool,\n",
        "                  nb_epochs = 3,\n",
        "                  nb_repeats = 1,\n",
        "                  window_size = 10,\n",
        "                  patience = 50)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaXEk2vEBGbM",
        "outputId": "ba5f478e-be2a-4d31-ab62-29e9307571e4"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run train.py"
      ],
      "metadata": {
        "id": "Lzmh8SM5n6dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "\n",
        "\n",
        "start_time = timer()\n",
        "!python going_modular/train.py\n",
        "end_time = timer()\n",
        "print(f\"Time to run train.py: {end_time-start_time:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "w97VvwvQDsgw",
        "outputId": "1b3a099c-fd84-40c9-be60-d238d4713a91"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No normalization for SPS. Feature removed!\n",
            "No normalization for AvgIpc. Feature removed!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
            "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
            "Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n",
            "Imported data_setup.py, took 6.7244\n",
            "Imported models.py, took 0.0005\n",
            "Imported engine.py, took 0.0005\n",
            "Imported utils.py, took 0.0003\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "time to import modules in data_setup.py: 6.724289792999116\n",
            "time to import graphdata in data_setup.py: 6.724268760999621\n",
            "Total import time: 10.712950606000959\n",
            "Created dataset, train_dataloader, test_dataloader, took 0.0733s\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Time to run train.py: 12.9665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbLRRwMaLyZT"
      },
      "execution_count": 136,
      "outputs": []
    }
  ]
}