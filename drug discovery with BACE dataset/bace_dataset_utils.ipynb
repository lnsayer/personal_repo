{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM09A2KyXATyogy1FSrUCeA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnsayer/personal_repo/blob/main/drug%20discovery%20with%20BACE%20dataset/bace_dataset_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Time how long it takes to install packages\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from IPython.display import Javascript\n",
        "\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "start_time = timer()\n",
        "rd_kit_start_time = timer()\n",
        "!pip install rdkit\n",
        "rd_kit_end_time = timer()\n",
        "\n",
        "torch_geometric_start_time = timer()\n",
        "!pip install torch_geometric\n",
        "torch_geometric_end_time = timer()\n",
        "\n",
        "deep_chem_start_time = timer()\n",
        "!pip install deepchem\n",
        "deep_chem_end_time = timer()\n",
        "\n",
        "networkx_start_time = timer()\n",
        "!pip install networkx\n",
        "networkx_end_time = timer()\n",
        "\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"Time for cell to run: {end_time-start_time:.4f}\")\n",
        "print(f\"rd_kit time: {rd_kit_end_time-rd_kit_start_time:.4f}\")\n",
        "print(f\"torch_geometric time: {torch_geometric_end_time-torch_geometric_start_time:.4f}\")\n",
        "print(f\"deep_chem time: {deep_chem_end_time-deep_chem_start_time:.4f}\")\n",
        "print(f\"networkx time: {networkx_end_time-networkx_start_time:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "6Nw4yVJfqkCL",
        "outputId": "df3aa6af-9a82-4f25-ce06-769b52c1039b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.3-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.3\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.8.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.13.0)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.11.4)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2024.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2024.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->deepchem) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: deepchem\n",
            "Successfully installed deepchem-2.8.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Time for cell to run: 52.1172\n",
            "rd_kit time: 26.0785\n",
            "torch_geometric time: 8.4410\n",
            "deep_chem time: 10.9629\n",
            "networkx time: 6.6346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import os.path as osp\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import InMemoryDataset, Dataset, Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, GraphConv, GATConv, MLP, GINConv, global_max_pool, SAGPooling, TopKPooling, GINEConv\n",
        "from torch.nn import Linear, ReLU, Dropout, Softmax\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import deepchem as dc\n",
        "from deepchem.feat.graph_data import GraphData\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from pandas import DataFrame\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "from IPython.display import Javascript\n",
        "import pickle\n",
        "\n",
        "from typing import Callable, Optional, Any\n",
        "\n",
        "\n",
        "import warnings"
      ],
      "metadata": {
        "id": "rdxCUz17sAa2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "70OHfycLpP_i"
      },
      "outputs": [],
      "source": [
        "# Custom torch geometric Dataset class to store the samples and their corresponding labels\n",
        "\n",
        "class MoleculeDataset(Dataset):\n",
        "  def __init__(self, root, csv_file, transform=None, pre_transform=None, pre_filter=None):\n",
        "    \"\"\"\n",
        "    Constructor method of the class\n",
        "\n",
        "    :root = Path where the dataset should be stored. This folder is split\n",
        "    into raw_dir (downloaded dataset) and processed_dir(processed data).\n",
        "    :csv_file = Desired name of the CSV file to be saved.\n",
        "    : transform, pre_transform, pre_filter = optional transforms\n",
        "    \"\"\"\n",
        "    self.csv_file = csv_file\n",
        "    super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    \"\"\"\n",
        "    If this file exists in raw_dir, the download is not triggered/\n",
        "    (the download function is not implemented here)\n",
        "    \"\"\"\n",
        "    return self.csv_file\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    \"\"\"\n",
        "    If these files are found in raw_dir, processing is skipped\n",
        "    \"\"\"\n",
        "    self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "\n",
        "    return [f'data_{i}.pt' for i in list(self.data.index)]\n",
        "\n",
        "  def download(self):\n",
        "    \"\"\"\n",
        "    No need to download the csv file as it is already downloaded\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def process(self):\n",
        "    \"\"\"\n",
        "    Converts molecules with SMILES formats into PyTorch graphs. Uses Deepchem's MolGraphConvFeaturizer to create a graph\n",
        "    and then convert that to a torch graph with to_pyg_graph. Saves these in the processed directory.\n",
        "    \"\"\"\n",
        "    self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "    featurizer=dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
        "\n",
        "    for idx, row in self.data.iterrows():\n",
        "      # Featurize molecule and convert to torch graph\n",
        "      smiles = row['mol']\n",
        "      label = row['Class']\n",
        "      pic50 = row['pIC50']\n",
        "\n",
        "      out = featurizer.featurize(smiles)\n",
        "      pyg_out = GraphData.to_pyg_graph(out[0])\n",
        "      pyg_out.Class = torch.tensor([label])\n",
        "      pyg_out.smiles = smiles\n",
        "      pyg_out.pic50 = pic50\n",
        "\n",
        "      # data = Data(x=pyg_out.x, edge_index=pyg_out.edge_index, edge_attr=pyg_out.edge_attr,\n",
        "      #            y=torch.tensor([label]), dtype = torch.float)\n",
        "\n",
        "      torch.save(pyg_out, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "\n",
        "  def len(self):\n",
        "    \"\"\"\n",
        "    Returns number of samples in the dataset\n",
        "    \"\"\"\n",
        "    return len(self.processed_file_names)\n",
        "\n",
        "  def get(self, idx):\n",
        "    \"\"\"\n",
        "    Loads a single graph\n",
        "    \"\"\"\n",
        "    data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Standard GCN graph classifier. Uses the graph convolutional operator from PyTorch geometric.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int, pool_method:torch_geometric.nn.pool):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Convolutional Layers\n",
        "    self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "\n",
        "    self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    # Linear layer used in classification\n",
        "    self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtain node embeddings\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.conv3(x, edge_index)\n",
        "\n",
        "    # Aggregate node embeddings\n",
        "    x = pool_method(x, batch)\n",
        "\n",
        "    # Regularisation\n",
        "    x = F.dropout(x)\n",
        "\n",
        "    # Classification\n",
        "    x = self.lin(x)\n",
        "\n",
        "    x = F.softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "GCNClassifier(7, 64, 2, global_mean_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is17by7rr54T",
        "outputId": "3048c564-bddf-4d0d-e7b2-9dea147fc165"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNClassifier(\n",
              "  (conv1): GCNConv(7, 64)\n",
              "  (conv2): GCNConv(64, 64)\n",
              "  (conv3): GCNConv(64, 64)\n",
              "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvClassifier(GCNClassifier):\n",
        "  \"\"\"\n",
        "  Same architecture as GCN Classifier however uses GraphConv layers\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int,  pool_method:torch_geometric.nn.pool):\n",
        "    super().__init__(in_channels, hidden_channels, out_channels, pool_method)\n",
        "    self.conv1 = GraphConv(in_channels, hidden_channels)\n",
        "\n",
        "    self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "GraphConvClassifier(7, 64, 2, global_mean_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNuRrU-dP-kM",
        "outputId": "03dccc3f-5dc5-47e5-eb8d-f91c17184bc6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphConvClassifier(\n",
              "  (conv1): GraphConv(7, 64)\n",
              "  (conv2): GraphConv(64, 64)\n",
              "  (conv3): GraphConv(64, 64)\n",
              "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GATClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  GAT Convolutional graph classifier. Uses the graph attention operator from PyTorch geometric\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int, heads:int, pool_method:torch_geometric.nn.pool,\n",
        "               use_edge_attr:bool):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : heads = the number of multi-headed attentions.\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    : use_edge_attribute = boolean variable which determines whether to use the edge attributes of the graph.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Convolutional Layers\n",
        "    self.conv1 = GATConv(in_channels,\n",
        "                         hidden_channels,\n",
        "                         heads,\n",
        "                         concat = True)\n",
        "    self.conv2 = GATConv(hidden_channels*heads,\n",
        "                         hidden_channels,\n",
        "                         heads,\n",
        "                         concat=True)\n",
        "    self.conv3 = GATConv(hidden_channels*heads,\n",
        "                         hidden_channels,\n",
        "                         1,\n",
        "                         concat=False)\n",
        "    self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtain node embeddings\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "\n",
        "    # can use edge attributes\n",
        "    if use_edge_attr:\n",
        "      x = self.conv1(x, edge_index, edge_attr)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv2(x, edge_index, edge_attr)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv3(x, edge_index, edge_attr)\n",
        "\n",
        "    # not using edge attributes\n",
        "    else:\n",
        "      x = self.conv1(x, edge_index)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv2(x, edge_index)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv3(x, edge_index)\n",
        "\n",
        "    # Aggregate node embeddings\n",
        "    x = pool_method(x, batch)\n",
        "\n",
        "    # Regularisation\n",
        "    x = F.dropout(x)\n",
        "\n",
        "    # Classification\n",
        "    x = self.lin(x)\n",
        "\n",
        "    x = F.softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "GATClassifier(30, 64, 2, 8, global_max_pool, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKrLQyJEQNfJ",
        "outputId": "1fb27dae-759b-46ea-e723-ea0a495c3029"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GATClassifier(\n",
              "  (conv1): GATConv(30, 64, heads=8)\n",
              "  (conv2): GATConv(512, 64, heads=8)\n",
              "  (conv3): GATConv(512, 64, heads=1)\n",
              "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GINConvClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Applies the graph isomorphism operator\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, pool_method: torch_geometric.nn.pool):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : num_layers = the number of layers of the multi-layer perceptron\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.conv = GINConv\n",
        "\n",
        "    # Create multiple GINConv layers as specified by num_layers\n",
        "    for _ in range(num_layers):\n",
        "      mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "      self.convs.append(self.conv(nn=mlp, train_eps=False))\n",
        "      in_channels = hidden_channels\n",
        "\n",
        "    # Define the final MLP\n",
        "    self.mlp = MLP([hidden_channels, hidden_channels, out_channels], norm = None, dropout = 0.5)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    \"\"\"\n",
        "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "    for conv in self.convs:\n",
        "      x = conv(x, edge_index).relu()\n",
        "    x = pool_method(x, batch)\n",
        "    return self.mlp(x)\n",
        "\n",
        "GINConvClassifier(30, 64, 2, 3, global_mean_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tIGZQrAQ2jI",
        "outputId": "0084e0fd-1bfc-456d-91f7-d66879d460a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GINConvClassifier(\n",
              "  (convs): ModuleList(\n",
              "    (0): GINConv(nn=MLP(30, 64, 64))\n",
              "    (1-2): 2 x GINConv(nn=MLP(64, 64, 64))\n",
              "  )\n",
              "  (mlp): MLP(64, 64, 2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GINEConvClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Same as the GINConvClassifier, however also uses edge attributes of the graphs\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, pool_method: torch_geometric.nn.pool,\n",
        "               use_edge_attr:bool, edge_dim:int):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : num_layers = the number of layers of the multi-layer perceptron\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    : use_edge_attr = boolean variable to determine whether will use the edge attributes or not.\n",
        "    : edge_dim = the dimensionality of the edge attributes for the graph's edges\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.conv = GINEConv\n",
        "    self.use_edge_attr = use_edge_attr\n",
        "    self.edge_dim = edge_dim\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "      self.convs.append(self.conv(nn=mlp, train_eps=False, edge_dim=self.edge_dim))\n",
        "      in_channels = hidden_channels\n",
        "\n",
        "    self.mlp = MLP([hidden_channels, hidden_channels, out_channels], norm = None, dropout = 0.5)\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "    for conv in self.convs:\n",
        "      if self.use_edge_attr:\n",
        "        x = conv(x, edge_index, edge_attr).relu()\n",
        "      else:\n",
        "        x = conv(x, edge_index).relu()\n",
        "\n",
        "    x = pool_method(x, batch)\n",
        "    return self.mlp(x)\n",
        "\n",
        "GINEConvClassifier(30, 64, 2, 3, global_mean_pool, True, 11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY2gJtGHSyN3",
        "outputId": "ecbe165c-31d1-44dc-8b02-2ed6a28675cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GINEConvClassifier(\n",
              "  (convs): ModuleList(\n",
              "    (0): GINEConv(nn=MLP(30, 64, 64))\n",
              "    (1-2): 2 x GINEConv(nn=MLP(64, 64, 64))\n",
              "  )\n",
              "  (mlp): MLP(64, 64, 2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model:torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "  \"\"\"\n",
        "  Performs the training of a model for one epoch for the training dataloader.\n",
        "\n",
        "  Returns lists of the training loss, accuracy and AUC of the training dataloader for the epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  train_loss, train_acc, train_auc = 0, 0, 0\n",
        "\n",
        "  # We time how long it takes for each section in the training process\n",
        "  auc_time = 0\n",
        "  out_time = 0\n",
        "  loss_time = 0\n",
        "  optimizer_time = 0\n",
        "  section_time = 0\n",
        "  dataloader_loop_time = 0\n",
        "  inside_loop_time = 0\n",
        "\n",
        "\n",
        "  loop_start_time = timer()\n",
        "\n",
        "  # Loop over the batches\n",
        "  for idx, batch in enumerate(dataloader):\n",
        "    # print(f\"entered {idx} loop of train step\")\n",
        "    inside_loop_start_time = timer()\n",
        "    # Time how long it takes to obtain an idx and batch of the dataloader\n",
        "    if idx > 1:\n",
        "      dataloader_loop_end_time = timer()\n",
        "      dataloader_loop_time += dataloader_loop_end_time-dataloader_loop_start_time\n",
        "\n",
        "    # Can time how long any chosen section takes to run\n",
        "    section_start_time = timer()\n",
        "    to_device_start_time = timer()\n",
        "    batch = batch.to(device)\n",
        "    to_device_end_time = timer()\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out_start_time = timer()\n",
        "    out = model(batch)\n",
        "    out_end_time = timer()\n",
        "    out_time+=out_end_time-out_start_time\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(out, batch.Class)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Calculate the label predictions\n",
        "    label_preds = torch.argmax(out, dim=1)\n",
        "    # Calculate accuracy\n",
        "    train_acc += (label_preds == batch.Class).sum()\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_start_time = timer()\n",
        "    # Check both classes present in batch.Class, otherwise add the batch_auc from the previous iteration\n",
        "    if len(torch.unique(batch.Class)) == 2:\n",
        "        batch_auc = roc_auc_score(batch.Class.detach().cpu().numpy(), out[:,1].detach().cpu().numpy())\n",
        "        train_auc += batch_auc\n",
        "    else:\n",
        "      train_auc += batch_auc\n",
        "\n",
        "    auc_end_time = timer()\n",
        "    auc_time += auc_end_time-auc_start_time\n",
        "\n",
        "\n",
        "    # Loss backward\n",
        "    loss_start_time = timer()\n",
        "    loss.backward()\n",
        "    loss_end_time = timer()\n",
        "    loss_time += loss_end_time-loss_start_time\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer_start_time = timer()\n",
        "    optimizer.step()\n",
        "    optimizer_end_time = timer()\n",
        "    optimizer_time = optimizer_end_time-optimizer_start_time\n",
        "    section_end_time = timer()\n",
        "    section_time+=section_end_time-section_start_time\n",
        "    dataloader_loop_start_time = timer()\n",
        "    inside_loop_end_time = timer()\n",
        "    inside_loop_time += inside_loop_end_time-inside_loop_start_time\n",
        "\n",
        "\n",
        "  loop_end_time = timer()\n",
        "  # print(f\"Section time is {section_time:.4f}\")\n",
        "  # print(f\"Dataloader loop time is {dataloader_loop_time:.4f}\")\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss/len(dataloader.dataset)\n",
        "  train_acc = train_acc/len(dataloader.dataset)\n",
        "  train_auc = train_auc/len(dataloader)\n",
        "  # print(f\"AUC calculation time: {auc_time:.4f}s, Forward pass: {out_time:.4f}s, Loss time: {loss_time:.4f}, Optimizer time: {optimizer_time:.4f}, To device time: {to_device_end_time-to_device_start_time:.4f}\\n\")\n",
        "\n",
        "\n",
        "  #print(f\"Train outside loop time is {loop_end_time-loop_start_time:.4f}, inside loop time is {inside_loop_time:.4f}\")\n",
        "\n",
        "  return train_loss, train_acc, train_auc"
      ],
      "metadata": {
        "id": "wkeTMm1FUOBn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model:torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs the testing of a model for one epoch for the test dataloader.\n",
        "\n",
        "  Returns lists of the test loss, accuracy and AUC of the test dataloader for the epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc, test_auc = 0, 0, 0\n",
        "\n",
        "  # Turn on torch inference manager\n",
        "  with torch.inference_mode():\n",
        "    # Loop through data batches\n",
        "    for idx, batch in enumerate(dataloader):\n",
        "      # print(f\"entered test step {idx} batch loop\")\n",
        "      batch = batch.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      out = model(batch)\n",
        "\n",
        "      # Calculate the loss\n",
        "      loss = loss_fn(out, batch.Class)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate the label predictions\n",
        "      label_preds = torch.argmax(out, dim=1)\n",
        "      # Calculate accuracy\n",
        "      test_acc += (label_preds == batch.Class).sum()/len(label_preds)\n",
        "\n",
        "      # Calculate the AUC\n",
        "      if len(torch.unique(batch.Class)) == 2:\n",
        "        batch_auc = roc_auc_score(batch.Class.detach().cpu().numpy(), out[:,1].detach().cpu().numpy())\n",
        "        test_auc += batch_auc\n",
        "      else:\n",
        "        test_auc += batch_auc\n",
        "\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss/len(dataloader)\n",
        "    test_acc = test_acc/len(dataloader)\n",
        "    test_auc = test_auc/len(dataloader)\n",
        "\n",
        "    return test_loss, test_acc, test_auc"
      ],
      "metadata": {
        "id": "b-oTM34IZeW3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average(values:list , window_size:int):\n",
        "    \"\"\"\n",
        "    Calculates the simple moving average of the last window_size elements in a list of values\n",
        "\n",
        "    Returns the average\n",
        "    \"\"\"\n",
        "    if len(values) < window_size:\n",
        "        return None\n",
        "    return sum(values[-window_size:]) / window_size"
      ],
      "metadata": {
        "id": "cp1YPhPFZzrV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n",
        "          epochs: int =5,\n",
        "          model_save_path: str = None,\n",
        "          window_size: int=10,\n",
        "          patience: int=10):\n",
        "  \"\"\"\n",
        "  Combines train_step and test_step to train a model and evaluate it at each epoch on the train_dataloader and test_dataloader.\n",
        "\n",
        "  :model_save_path = a string used to save the model's parameters. If no string is provided it is not saved.\n",
        "\n",
        "  Returns a dictionary of results.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # 1. Create empty results dictionary\n",
        "  results = {\"epoch\": [],\n",
        "             \"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"train_auc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": [],\n",
        "             \"test_auc\": [],\n",
        "             \"saved_epochs\": [],\n",
        "             \"test_loss_mov_avg\": [],\n",
        "             \"test_auc_mov_avg\": []}\n",
        "  # 2. Loop through training and testing steps for a number of epochs\n",
        "  best_moving_loss_avg = float('inf')\n",
        "  best_moving_auc_avg = 0\n",
        "\n",
        "  # Loop over the number of epochs\n",
        "  for i in tqdm(range(epochs)):\n",
        "\n",
        "    start_time = timer()\n",
        "    train_step_start_time = timer()\n",
        "    train_loss, train_acc, train_auc = train_step(model,\n",
        "                                       train_dataloader,\n",
        "                                       loss_fn,\n",
        "                                       optimizer)\n",
        "    train_step_end_time = timer()\n",
        "    test_step_start_time = timer()\n",
        "    test_loss, test_acc, test_auc = test_step(model,\n",
        "                                    test_dataloader,\n",
        "                                    loss_fn,\n",
        "                                    optimizer)\n",
        "    test_step_end_time = timer()\n",
        "    # print(f\"Train step time is {train_step_end_time-train_step_start_time:.4f}s, Test step time is {test_step_end_time-test_step_start_time:.4f}s\\n\")\n",
        "\n",
        "    # 3. Print out what's happening\n",
        "    print(f\"Epoch: {i}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Train AUC: {train_auc:.4f}, Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}, Test auc: {test_auc:.4f}\")\n",
        "    # 4. Update results dictionary\n",
        "    append_start_time = timer()\n",
        "    results[\"epoch\"].append(i)\n",
        "    results[\"train_loss\"].append(round(train_loss, 4))\n",
        "    results[\"train_acc\"].append(round(train_acc.item(), 4))\n",
        "    results[\"train_auc\"].append(round(train_auc, 4))\n",
        "    results[\"test_loss\"].append(round(test_loss, 4))\n",
        "    results[\"test_acc\"].append(round(test_acc.item(), 4))\n",
        "    results[\"test_auc\"].append(round(test_auc, 4))\n",
        "    append_end_time = timer()\n",
        "    # print(f\"append time is{append_end_time-append_start_time:.4f}\")\n",
        "\n",
        "    # 5. If model_save_path provided, save the model to its path based on whether test loss and test AUC have improved.\n",
        "    \"\"\"\n",
        "    Once the number of epochs is greater than the window_size a current moving average is created of the last 'window_size'\n",
        "    loss and AUC values. As long as these current metrics are higher than the current moving average, the model is saved.\n",
        "    If the current metrics are not better than the current moving average for 'patience' epochs the training stops early.\n",
        "    \"\"\"\n",
        "\n",
        "    save_timer_start = timer()\n",
        "    if model_save_path:\n",
        "      current_moving_loss_avg = moving_average(results[\"test_loss\"], window_size)\n",
        "      if current_moving_loss_avg is not None:\n",
        "        results[\"test_loss_mov_avg\"].append(round(current_moving_loss_avg, 4))\n",
        "      else:\n",
        "        results[\"test_loss_mov_avg\"].append(None)\n",
        "\n",
        "\n",
        "      current_moving_auc_avg = moving_average(results[\"test_auc\"], window_size)\n",
        "      if current_moving_auc_avg is not None:\n",
        "        results[\"test_auc_mov_avg\"].append(round(current_moving_auc_avg, 4))\n",
        "      else:\n",
        "        results[\"test_auc_mov_avg\"].append(None)\n",
        "\n",
        "      if current_moving_loss_avg is not None and current_moving_auc_avg is not None:\n",
        "        if current_moving_loss_avg < best_moving_loss_avg and current_moving_auc_avg > best_moving_auc_avg:\n",
        "          without_improvement_count = 0\n",
        "          results[\"saved_epochs\"].append(i)\n",
        "          torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
        "                   f=model_save_path)\n",
        "          print(f\"Saved model at epoch {i} with current average test loss: {current_moving_loss_avg:.4f} and previous best: {best_moving_loss_avg:.4f}\")\n",
        "          print(f\"Saved model at epoch {i} with current average AUC loss: {current_moving_auc_avg:.4f} and previous best: {best_moving_auc_avg:.4f}\")\n",
        "          best_moving_loss_avg = current_moving_loss_avg\n",
        "          best_moving_auc_avg = current_moving_auc_avg\n",
        "\n",
        "        else:\n",
        "          without_improvement_count += 1\n",
        "          print(f\"Without_improvement_count: {without_improvement_count}\")\n",
        "        if without_improvement_count > patience:\n",
        "          print(\"Early Stopping\")\n",
        "          break\n",
        "    save_timer_end = timer()\n",
        "    end_time  = timer()\n",
        "    print(f\"Epoch took {end_time-start_time:.2f} seconds\")\n",
        "    # print(f\"Time to save loop : {save_timer_end-save_timer_start:.4f}\")\n",
        "\n",
        "  # 6. Return the filled results at the end of the epochs\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "Ec10XN0Zack5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model_repeats(model_architecture: Callable[[], torch.nn.Module],\n",
        "                      optimizer_: Callable[[], torch.optim.Optimizer],\n",
        "                      criterion_: torch.nn.Module,\n",
        "                      models_directory: Path=None,\n",
        "                      num_hidden_channels: int = 128,\n",
        "                      pool_method: Any = global_mean_pool,\n",
        "                      nb_epochs: int = 300,\n",
        "                      nb_repeats: int = 1,\n",
        "                      window_size: int = 10,\n",
        "                      patience: int = 50):\n",
        "  \"\"\"\n",
        "  Runs training runs for 'nb_repeats' and optionally saves them if nb_repeats.\n",
        "  Optionally save the model and its results if models_directory provided.\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(nb_repeats):\n",
        "    if models_directory:\n",
        "      model_save_name = f\"{i}_{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}.pth\"\n",
        "      model_save_path  = models_directory / model_save_name\n",
        "    else:\n",
        "      model_save_path = None\n",
        "    model = model_architecture()\n",
        "    optimizer = optimizer_(model.parameters())\n",
        "\n",
        "\n",
        "    results = train(model,\n",
        "        train_dataloader,\n",
        "        test_dataloader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        epochs = nb_epochs,\n",
        "        model_save_path = model_save_path,\n",
        "        window_size=window_size,\n",
        "        patience=patience)\n",
        "    if models_directory:\n",
        "      with open(models_directory/f\"{i}_{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_results.pkl\", 'wb') as f:\n",
        "        print(\"Saved results of this model\")\n",
        "        pickle.dump(results, f)\n",
        "    else:\n",
        "      print(\"Did not save results of this model\")"
      ],
      "metadata": {
        "id": "N4sgPv9JcyqD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callables for run_model_repeats\n",
        "\n",
        "def gcn_callable():\n",
        "    return GCNClassifier(num_features, num_hidden_channels, num_out_channels, pool_method)\n",
        "\n",
        "def gat_callable():\n",
        "    return GATClassifier(num_features, num_hidden_channels, num_out_channels, heads, pool_method, use_edge_attr)\n",
        "\n",
        "def graphconv_callable():\n",
        "    return GraphConvClassifier(num_features, num_hidden_channels, num_out_channels, pool_method)\n",
        "\n",
        "def ginconv_callable():\n",
        "    return GINConvClassifier(num_features, num_hidden_channels, num_out_channels, num_layers, pool_method)\n",
        "\n",
        "def adam_optimizer_callable(parameters, lr=0.001, weight_decay=0):\n",
        "    return torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "W0IaKbqvFilV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_metric_func(model, train_dataloader, test_dataloader):\n",
        "  \"\"\"\n",
        "  Calculates the metrics (AUC and Scikit-Learn's Classification report metrics) for a model for its training data and test data.\n",
        "\n",
        "  Returns training and test AUROC curve and training and test Sklearn classification report as a dataframe\n",
        "  \"\"\"\n",
        "  with torch.inference_mode():\n",
        "    model.eval()\n",
        "\n",
        "    # Create empty tensors to fill with probabilities, predictions and labels\n",
        "    total_train_probs = torch.empty(len(train_dataset))\n",
        "    total_train_preds = torch.empty(len(train_dataset))\n",
        "    total_train_labels = torch.empty(len(train_dataset))\n",
        "\n",
        "    total_test_probs = torch.empty(len(test_dataset))\n",
        "    total_test_preds = torch.empty(len(test_dataset))\n",
        "    total_test_labels = torch.empty(len(test_dataset))\n",
        "\n",
        "    # Loop over batches and add to the total tensors\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "      batch = batch.to(device)\n",
        "      batch_size = 32\n",
        "      current_batch_size = len(batch)\n",
        "      # print(f\"{idx*batch_size}:{idx*batch_size+current_batch_size}\")\n",
        "      out = model.forward(batch)\n",
        "      train_probs = out[:,1]\n",
        "      train_preds = torch.argmax(out, dim=1)\n",
        "\n",
        "      total_train_probs[idx*batch_size:idx*batch_size+current_batch_size] = train_probs\n",
        "      total_train_preds[idx*batch_size:idx*batch_size+current_batch_size] = train_preds\n",
        "      total_train_labels[idx*batch_size:idx*batch_size+current_batch_size] = batch.Class\n",
        "\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "      batch = batch.to(device)\n",
        "      batch_size = 32\n",
        "      current_batch_size = len(batch)\n",
        "      # print(f\"{idx*batch_size}:{idx*batch_size+current_batch_size}\")\n",
        "      out = model.forward(batch)\n",
        "      test_probs = out[:,1]\n",
        "      test_preds = torch.argmax(out, dim=1)\n",
        "\n",
        "      total_test_probs[idx*batch_size:idx*batch_size+current_batch_size] = test_probs\n",
        "      total_test_preds[idx*batch_size:idx*batch_size+current_batch_size] = test_preds\n",
        "      total_test_labels[idx*batch_size:idx*batch_size+current_batch_size] = batch.Class\n",
        "\n",
        "    # Calculate AUC and dataframes of metrics (using Scikit-Learn's metrics)\n",
        "    train_auroc = roc_auc_score(total_train_labels, total_train_probs).item()\n",
        "    train_classification_report = classification_report(total_train_labels, total_train_preds, output_dict=True)\n",
        "    train_report_df = pd.DataFrame(data=train_classification_report).transpose()\n",
        "\n",
        "    test_auroc = roc_auc_score(total_test_labels, total_test_probs).item()\n",
        "    test_classification_report = classification_report(total_test_labels, total_test_preds, output_dict=True)\n",
        "    test_report_df = pd.DataFrame(data=test_classification_report).transpose()\n",
        "\n",
        "\n",
        "  return train_auroc, test_auroc, train_report_df, test_report_df"
      ],
      "metadata": {
        "id": "CC8cK7GLGVCq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_model_metrics(model: torch.nn.Module,\n",
        "                          models_directory: Path,\n",
        "                          model_name_stem: str,\n",
        "                          repeats: int,\n",
        "                          save_yes_no: bool):\n",
        "  \"\"\"\n",
        "  Takes a GNN model architecture and a directory where its weights are stored (repeats) then return an average of their performance metrics.\n",
        "  Uses new_metric_func\n",
        "\n",
        "  :model = instance of a classification model which will load the model parameters of trained models\n",
        "  :model_directory = directory where the model parameters are stored\n",
        "  : model_name_stem = name stem of the repeats e.g. \"_128_300_global_mean_pool.pth\"\n",
        "  : repeats = number of repeats in the directory\n",
        "  : save_yes_no = option to save the results as pickle files in the same directory\n",
        "\n",
        "  Returns dataframes of the mean and stadndard deviation of the training and test performance metrics (AUROC, Sklearn metrics).\n",
        "  \"\"\"\n",
        "  # Will append results of each model's performance to these lists (AUROC and classification report metric)\n",
        "  train_auroc_list = []\n",
        "  test_auroc_list = []\n",
        "  train_report_list = []\n",
        "  test_report_list = []\n",
        "\n",
        "  # Loop over the number of repeats of the model runs\n",
        "  for i in range(repeats):\n",
        "    model_name = f\"{i}\" + model_name_stem\n",
        "    model_path = models_directory / model_name\n",
        "    model.load_state_dict(torch.load(f=model_path))\n",
        "    model.to(device)\n",
        "    train_auroc, test_auroc, train_classification_report, test_classification_report = new_metric_func(model, train_dataloader, test_dataloader)\n",
        "    train_auroc_list.append(train_auroc)\n",
        "    test_auroc_list.append(test_auroc)\n",
        "    train_report_list.append(pd.DataFrame(train_classification_report))\n",
        "    test_report_list.append(pd.DataFrame(test_classification_report))\n",
        "\n",
        "  # Calculate averages and standard deviations of our repeats\n",
        "  mean_train_auroc = np.mean(train_auroc_list)\n",
        "  mean_test_auroc = np.mean(test_auroc_list)\n",
        "\n",
        "  std_train_auroc = np.std(train_auroc_list)\n",
        "  std_test_auroc = np.std(test_auroc_list)\n",
        "\n",
        "  auroc_data = {\"Train\": [mean_train_auroc, std_train_auroc],\n",
        "               \"Test\": [mean_test_auroc, std_test_auroc]}\n",
        "  auroc_df = pd.DataFrame(auroc_data)\n",
        "\n",
        "\n",
        "  mean_train_model_metrics = pd.DataFrame(pd.concat(train_report_list).groupby(level=0).mean())\n",
        "  mean_test_model_metrics = pd.DataFrame(pd.concat(test_report_list).groupby(level=0).mean())\n",
        "\n",
        "  std_train_model_metrics = pd.DataFrame(pd.concat(train_report_list).groupby(level=0).std())\n",
        "  std_test_model_metrics = pd.DataFrame(pd.concat(test_report_list).groupby(level=0).std())\n",
        "\n",
        "  # Option to save the results as pickle files (which can later be loaded and turned into dataframes)\n",
        "  if save_yes_no:\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_auroc_df.pkl\", 'wb') as f:\n",
        "      print(\"Saved auroc dataframe\")\n",
        "      pickle.dump(auroc_df, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_mean_train_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved mean train metrics\")\n",
        "      pickle.dump(mean_train_model_metrics, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_mean_test_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved mean test metrics\")\n",
        "      pickle.dump(mean_test_model_metrics, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_std_train_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved std train metrics\")\n",
        "      pickle.dump(std_train_model_metrics, f)\n",
        "\n",
        "    with open(models_directory/f\"{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_std_test_metrics.pkl\", 'wb') as f:\n",
        "      print(\"Saved std test metrics\")\n",
        "      pickle.dump(std_test_model_metrics, f)\n",
        "\n",
        "\n",
        "  return auroc_df, mean_train_model_metrics, mean_test_model_metrics, std_train_model_metrics, std_test_model_metrics"
      ],
      "metadata": {
        "id": "CZJ9jmPzHMUY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_acc_auc_plots(results:dict):\n",
        "  \"\"\"\n",
        "  Plots the loss, accuracy metric and AUCROC metric curves of the training and test set of a model results (dictionary).\n",
        "\n",
        "  Returns three plots.\n",
        "  \"\"\"\n",
        "\n",
        "  fig, ax = plt.subplots(ncols=3, nrows=1, figsize = (15,6))\n",
        "\n",
        "  ax[0].plot(results[\"epoch\"], results[\"train_loss\"], label=\"Train\");\n",
        "  ax[0].plot(results[\"epoch\"], results[\"test_loss\"],  label=\"Test\");\n",
        "  ax[0].vlines(results[\"saved_epochs\"], ymin=0, ymax = max(results[\"test_loss\"]), alpha=0.2, colors='black', linestyle='dashed', label = 'Saved epochs')\n",
        "\n",
        "  ax[1].plot(results[\"epoch\"], results[\"train_acc\"], label=\"Train\");\n",
        "  ax[1].plot(results[\"epoch\"], results[\"test_acc\"],  label=\"Test\");\n",
        "  ax[1].vlines(results[\"saved_epochs\"], ymin=0, ymax = max(results[\"train_acc\"]), alpha=0.2, colors='black', linestyle='dashed', label = 'Saved epochs')\n",
        "\n",
        "  ax[2].plot(results[\"epoch\"], results[\"train_auc\"], label=\"Train\");\n",
        "  ax[2].plot(results[\"epoch\"], results[\"test_auc\"],  label=\"Test\");\n",
        "  ax[2].vlines(results[\"saved_epochs\"], ymin=0, ymax = max(results[\"train_auc\"]), alpha=0.2, colors='black', linestyle='dashed', label = 'Saved epochs')\n",
        "\n",
        "  ax[0].set_xlabel(\"Epochs\", size=14)\n",
        "  ax[0].set_ylabel(\"Loss\", size=14)\n",
        "\n",
        "  ax[1].set_xlabel(\"Epochs\", size=14)\n",
        "  ax[1].set_ylabel(\"Accuracy\", size=14)\n",
        "\n",
        "  ax[2].set_xlabel(\"Epochs\", size=14)\n",
        "  ax[2].set_ylabel(\"AUC\", size=14)\n",
        "\n",
        "  ax[0].legend();\n",
        "  ax[1].legend();\n",
        "  ax[2].legend();"
      ],
      "metadata": {
        "id": "PjtcMGGEKGlv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLX2GKkCMrbI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}