{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaLgo3waUANaft3YDjWep2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnsayer/personal_repo/blob/main/drug%20discovery%20with%20BACE%20dataset/bace_dataset_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Time how long it takes to install packages\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from IPython.display import Javascript\n",
        "\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "start_time = timer()\n",
        "rd_kit_start_time = timer()\n",
        "!pip install rdkit\n",
        "rd_kit_end_time = timer()\n",
        "\n",
        "torch_geometric_start_time = timer()\n",
        "!pip install torch_geometric\n",
        "torch_geometric_end_time = timer()\n",
        "\n",
        "deep_chem_start_time = timer()\n",
        "!pip install deepchem\n",
        "deep_chem_end_time = timer()\n",
        "\n",
        "networkx_start_time = timer()\n",
        "!pip install networkx\n",
        "networkx_end_time = timer()\n",
        "\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"Time for cell to run: {end_time-start_time:.4f}\")\n",
        "print(f\"rd_kit time: {rd_kit_end_time-rd_kit_start_time:.4f}\")\n",
        "print(f\"torch_geometric time: {torch_geometric_end_time-torch_geometric_start_time:.4f}\")\n",
        "print(f\"deep_chem time: {deep_chem_end_time-deep_chem_start_time:.4f}\")\n",
        "print(f\"networkx time: {networkx_end_time-networkx_start_time:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nw4yVJfqkCL",
        "outputId": "0cf9898a-29f4-4f91-e750-19b59b964327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.3-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.3\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.8.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.12.1)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.11.4)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2024.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2024.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->deepchem) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: deepchem\n",
            "Successfully installed deepchem-2.8.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n",
            "Time for cell to run: 43.1148\n",
            "rd_kit time: 16.9394\n",
            "torch_geometric time: 9.0694\n",
            "deep_chem time: 10.5635\n",
            "networkx time: 6.5422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import os.path as osp\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import InMemoryDataset, Dataset, Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, GraphConv, GATConv, MLP, GINConv, global_max_pool, SAGPooling, TopKPooling, GINEConv\n",
        "from torch.nn import Linear, ReLU, Dropout, Softmax\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import deepchem as dc\n",
        "from deepchem.feat.graph_data import GraphData\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from pandas import DataFrame\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import random\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "from IPython.display import Javascript\n",
        "import pickle\n",
        "\n",
        "import warnings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdxCUz17sAa2",
        "outputId": "0f77603c-eb6a-45f3-b512-b52cafbb861e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for SPS. Feature removed!\n",
            "WARNING:deepchem.feat.molecule_featurizers.rdkit_descriptors:No normalization for AvgIpc. Feature removed!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70OHfycLpP_i"
      },
      "outputs": [],
      "source": [
        "# Custom torch geometric Dataset class to store the samples and their corresponding labels\n",
        "\n",
        "class MoleculeDataset(Dataset):\n",
        "  def __init__(self, root, csv_file, transform=None, pre_transform=None, pre_filter=None):\n",
        "    \"\"\"\n",
        "    Constructor method of the class\n",
        "\n",
        "    :root = Path where the dataset should be stored. This folder is split\n",
        "    into raw_dir (downloaded dataset) and processed_dir(processed data).\n",
        "    :csv_file = Desired name of the CSV file to be saved.\n",
        "    : transform, pre_transform, pre_filter = optional transforms\n",
        "    \"\"\"\n",
        "    self.csv_file = csv_file\n",
        "    super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    \"\"\"\n",
        "    If this file exists in raw_dir, the download is not triggered/\n",
        "    (the download function is not implemented here)\n",
        "    \"\"\"\n",
        "    return self.csv_file\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    \"\"\"\n",
        "    If these files are found in raw_dir, processing is skipped\n",
        "    \"\"\"\n",
        "    self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "\n",
        "    return [f'data_{i}.pt' for i in list(self.data.index)]\n",
        "\n",
        "  def download(self):\n",
        "    \"\"\"\n",
        "    No need to download the csv file as it is already downloaded\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def process(self):\n",
        "    \"\"\"\n",
        "    Converts molecules with SMILES formats into PyTorch graphs. Uses Deepchem's MolGraphConvFeaturizer to create a graph\n",
        "    and then convert that to a torch graph with to_pyg_graph. Saves these in the processed directory.\n",
        "    \"\"\"\n",
        "    self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "    featurizer=dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
        "\n",
        "    for idx, row in self.data.iterrows():\n",
        "      # Featurize molecule and convert to torch graph\n",
        "      smiles = row['mol']\n",
        "      label = row['Class']\n",
        "      pic50 = row['pIC50']\n",
        "\n",
        "      out = featurizer.featurize(smiles)\n",
        "      pyg_out = GraphData.to_pyg_graph(out[0])\n",
        "      pyg_out.Class = torch.tensor([label])\n",
        "      pyg_out.smiles = smiles\n",
        "      pyg_out.pic50 = pic50\n",
        "\n",
        "      # data = Data(x=pyg_out.x, edge_index=pyg_out.edge_index, edge_attr=pyg_out.edge_attr,\n",
        "      #            y=torch.tensor([label]), dtype = torch.float)\n",
        "\n",
        "      torch.save(pyg_out, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "\n",
        "  def len(self):\n",
        "    \"\"\"\n",
        "    Returns number of samples in the dataset\n",
        "    \"\"\"\n",
        "    return len(self.processed_file_names)\n",
        "\n",
        "  def get(self, idx):\n",
        "    \"\"\"\n",
        "    Loads a single graph\n",
        "    \"\"\"\n",
        "    data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Standard GCN graph classifier. Uses the graph convolutional operator from PyTorch geometric.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int, pool_method:torch_geometric.nn.pool):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Convolutional Layers\n",
        "    self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "\n",
        "    self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    # Linear layer used in classification\n",
        "    self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtain node embeddings\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.leaky_relu(x)\n",
        "    x = self.conv3(x, edge_index)\n",
        "\n",
        "    # Aggregate node embeddings\n",
        "    x = pool_method(x, batch)\n",
        "\n",
        "    # Regularisation\n",
        "    x = F.dropout(x)\n",
        "\n",
        "    # Classification\n",
        "    x = self.lin(x)\n",
        "\n",
        "    x = F.softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "GCNClassifier(7, 64, 2, global_mean_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is17by7rr54T",
        "outputId": "649b82dd-4536-473a-d962-e7c453c50165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNClassifier(\n",
              "  (conv1): GCNConv(7, 64)\n",
              "  (conv2): GCNConv(64, 64)\n",
              "  (conv3): GCNConv(64, 64)\n",
              "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvClassifier(GCNClassifier):\n",
        "  \"\"\"\n",
        "  Same architecture as GCN Classifier however uses GraphConv layers\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int,  pool_method:torch_geometric.nn.pool):\n",
        "    super().__init__(in_channels, hidden_channels, out_channels, pool_method)\n",
        "    self.conv1 = GraphConv(in_channels, hidden_channels)\n",
        "\n",
        "    self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "GraphConvClassifier(7, 64, 2, global_mean_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNuRrU-dP-kM",
        "outputId": "5716fe04-35ca-4ca0-810a-7459d44231eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphConvClassifier(\n",
              "  (conv1): GraphConv(7, 64)\n",
              "  (conv2): GraphConv(64, 64)\n",
              "  (conv3): GraphConv(64, 64)\n",
              "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GATClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  GAT Convolutional graph classifier. Uses the graph attention operator from PyTorch geometric\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels:int, hidden_channels:int, out_channels:int, heads:int, pool_method:torch_geometric.nn.pool,\n",
        "               use_edge_attr:bool):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : heads = the number of multi-headed attentions.\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    : use_edge_attribute = boolean variable which determines whether to use the edge attributes of the graph.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Convolutional Layers\n",
        "    self.conv1 = GATConv(in_channels,\n",
        "                         hidden_channels,\n",
        "                         heads,\n",
        "                         concat = True)\n",
        "    self.conv2 = GATConv(hidden_channels*heads,\n",
        "                         hidden_channels,\n",
        "                         heads,\n",
        "                         concat=True)\n",
        "    self.conv3 = GATConv(hidden_channels*heads,\n",
        "                         hidden_channels,\n",
        "                         1,\n",
        "                         concat=False)\n",
        "    self.lin = Linear(hidden_channels, out_channels)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtain node embeddings\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "\n",
        "    # can use edge attributes\n",
        "    if use_edge_attr:\n",
        "      x = self.conv1(x, edge_index, edge_attr)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv2(x, edge_index, edge_attr)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv3(x, edge_index, edge_attr)\n",
        "\n",
        "    # not using edge attributes\n",
        "    else:\n",
        "      x = self.conv1(x, edge_index)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv2(x, edge_index)\n",
        "      x = F.leaky_relu(x)\n",
        "      x = self.conv3(x, edge_index)\n",
        "\n",
        "    # Aggregate node embeddings\n",
        "    x = pool_method(x, batch)\n",
        "\n",
        "    # Regularisation\n",
        "    x = F.dropout(x)\n",
        "\n",
        "    # Classification\n",
        "    x = self.lin(x)\n",
        "\n",
        "    x = F.softmax(x, dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "GATClassifier(30, 64, 2, 8, global_max_pool, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKrLQyJEQNfJ",
        "outputId": "8d5b6704-6571-487c-dd3e-5a07e85858ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GATClassifier(\n",
              "  (conv1): GATConv(30, 64, heads=8)\n",
              "  (conv2): GATConv(512, 64, heads=8)\n",
              "  (conv3): GATConv(512, 64, heads=1)\n",
              "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GINConvClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Applies the graph isomorphism operator\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, pool_method: torch_geometric.nn.pool):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : num_layers = the number of layers of the multi-layer perceptron\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.conv = GINConv\n",
        "\n",
        "    # Create multiple GINConv layers as specified by num_layers\n",
        "    for _ in range(num_layers):\n",
        "      mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "      self.convs.append(self.conv(nn=mlp, train_eps=False))\n",
        "      in_channels = hidden_channels\n",
        "\n",
        "    # Define the final MLP\n",
        "    self.mlp = MLP([hidden_channels, hidden_channels, out_channels], norm = None, dropout = 0.5)\n",
        "\n",
        "  def forward(self, data):\n",
        "    \"\"\"\n",
        "    Forward pass of the network\n",
        "    :data = the input data containing node features, edge indices, and batch information\n",
        "    \"\"\"\n",
        "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "    for conv in self.convs:\n",
        "      x = conv(x, edge_index).relu()\n",
        "    x = pool_method(x, batch)\n",
        "    return self.mlp(x)\n",
        "\n",
        "GINConvClassifier(30, 64, 2, 3, global_mean_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tIGZQrAQ2jI",
        "outputId": "988966be-2e14-40da-b173-19bf97456de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GINConvClassifier(\n",
              "  (convs): ModuleList(\n",
              "    (0): GINConv(nn=MLP(30, 64, 64))\n",
              "    (1-2): 2 x GINConv(nn=MLP(64, 64, 64))\n",
              "  )\n",
              "  (mlp): MLP(64, 64, 2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GINEConvClassifier(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Same as the GINConvClassifier, however also uses edge attributes of the graphs\n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels, hidden_channels, out_channels, num_layers, pool_method: torch_geometric.nn.pool,\n",
        "               use_edge_attr:bool, edge_dim:int):\n",
        "    \"\"\"\n",
        "    Constructor method\n",
        "    :in_channels = number of features of the graph's nodes\n",
        "    : hidden_channels = the number of hidden neurons in the network. The \"width\" of the network\n",
        "    : out_channels = the number of output features, i.e 2 for classification.\n",
        "    : num_layers = the number of layers of the multi-layer perceptron\n",
        "    : pool_method = the pooling method to obtain graph embedding from node embedding.\n",
        "    : use_edge_attr = boolean variable to determine whether will use the edge attributes or not.\n",
        "    : edge_dim = the dimensionality of the edge attributes for the graph's edges\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.convs = torch.nn.ModuleList()\n",
        "    self.conv = GINEConv\n",
        "    self.use_edge_attr = use_edge_attr\n",
        "    self.edge_dim = edge_dim\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "      mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
        "      self.convs.append(self.conv(nn=mlp, train_eps=False, edge_dim=self.edge_dim))\n",
        "      in_channels = hidden_channels\n",
        "\n",
        "    self.mlp = MLP([hidden_channels, hidden_channels, out_channels], norm = None, dropout = 0.5)\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index, batch, edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
        "    for conv in self.convs:\n",
        "      if self.use_edge_attr:\n",
        "        x = conv(x, edge_index, edge_attr).relu()\n",
        "      else:\n",
        "        x = conv(x, edge_index).relu()\n",
        "\n",
        "    x = pool_method(x, batch)\n",
        "    return self.mlp(x)\n",
        "\n",
        "GINEConvClassifier(30, 64, 2, 3, global_mean_pool, True, 11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY2gJtGHSyN3",
        "outputId": "1cdd0872-4430-45ec-fbcc-401fe4e4cf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GINEConvClassifier(\n",
              "  (convs): ModuleList(\n",
              "    (0): GINEConv(nn=MLP(30, 64, 64))\n",
              "    (1-2): 2 x GINEConv(nn=MLP(64, 64, 64))\n",
              "  )\n",
              "  (mlp): MLP(64, 64, 2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model:torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "  \"\"\"\n",
        "  Performs the training of a model for one epoch for the training dataloader.\n",
        "\n",
        "  Returns lists of the training loss, accuracy and AUC of the training dataloader for the epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  train_loss, train_acc, train_auc = 0, 0, 0\n",
        "\n",
        "  # We time how long it takes for each section in the training process\n",
        "  auc_time = 0\n",
        "  out_time = 0\n",
        "  loss_time = 0\n",
        "  optimizer_time = 0\n",
        "  section_time = 0\n",
        "  dataloader_loop_time = 0\n",
        "  inside_loop_time = 0\n",
        "\n",
        "\n",
        "  loop_start_time = timer()\n",
        "\n",
        "  # Loop over the batches\n",
        "  for idx, batch in enumerate(dataloader):\n",
        "    # print(f\"entered {idx} loop of train step\")\n",
        "    inside_loop_start_time = timer()\n",
        "    # Time how long it takes to obtain an idx and batch of the dataloader\n",
        "    if idx > 1:\n",
        "      dataloader_loop_end_time = timer()\n",
        "      dataloader_loop_time += dataloader_loop_end_time-dataloader_loop_start_time\n",
        "\n",
        "    # Can time how long any chosen section takes to run\n",
        "    section_start_time = timer()\n",
        "    to_device_start_time = timer()\n",
        "    batch = batch.to(device)\n",
        "    to_device_end_time = timer()\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out_start_time = timer()\n",
        "    out = model(batch)\n",
        "    out_end_time = timer()\n",
        "    out_time+=out_end_time-out_start_time\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(out, batch.Class)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Calculate the label predictions\n",
        "    label_preds = torch.argmax(out, dim=1)\n",
        "    # Calculate accuracy\n",
        "    train_acc += (label_preds == batch.Class).sum()\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc_start_time = timer()\n",
        "    # Check both classes present in batch.Class, otherwise add the batch_auc from the previous iteration\n",
        "    if len(torch.unique(batch.Class)) == 2:\n",
        "        batch_auc = roc_auc_score(batch.Class.detach().cpu().numpy(), out[:,1].detach().cpu().numpy())\n",
        "        train_auc += batch_auc\n",
        "    else:\n",
        "      train_auc += batch_auc\n",
        "\n",
        "    auc_end_time = timer()\n",
        "    auc_time += auc_end_time-auc_start_time\n",
        "\n",
        "\n",
        "    # Loss backward\n",
        "    loss_start_time = timer()\n",
        "    loss.backward()\n",
        "    loss_end_time = timer()\n",
        "    loss_time += loss_end_time-loss_start_time\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer_start_time = timer()\n",
        "    optimizer.step()\n",
        "    optimizer_end_time = timer()\n",
        "    optimizer_time = optimizer_end_time-optimizer_start_time\n",
        "    section_end_time = timer()\n",
        "    section_time+=section_end_time-section_start_time\n",
        "    dataloader_loop_start_time = timer()\n",
        "    inside_loop_end_time = timer()\n",
        "    inside_loop_time += inside_loop_end_time-inside_loop_start_time\n",
        "\n",
        "\n",
        "  loop_end_time = timer()\n",
        "  # print(f\"Section time is {section_time:.4f}\")\n",
        "  # print(f\"Dataloader loop time is {dataloader_loop_time:.4f}\")\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss/len(dataloader.dataset)\n",
        "  train_acc = train_acc/len(dataloader.dataset)\n",
        "  train_auc = train_auc/len(dataloader)\n",
        "  # print(f\"AUC calculation time: {auc_time:.4f}s, Forward pass: {out_time:.4f}s, Loss time: {loss_time:.4f}, Optimizer time: {optimizer_time:.4f}, To device time: {to_device_end_time-to_device_start_time:.4f}\\n\")\n",
        "\n",
        "\n",
        "  #print(f\"Train outside loop time is {loop_end_time-loop_start_time:.4f}, inside loop time is {inside_loop_time:.4f}\")\n",
        "\n",
        "  return train_loss, train_acc, train_auc"
      ],
      "metadata": {
        "id": "wkeTMm1FUOBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model:torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs the testing of a model for one epoch for the test dataloader.\n",
        "\n",
        "  Returns lists of the test loss, accuracy and AUC of the test dataloader for the epoch.\n",
        "  \"\"\"\n",
        "\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc, test_auc = 0, 0, 0\n",
        "\n",
        "  # Turn on torch inference manager\n",
        "  with torch.inference_mode():\n",
        "    # Loop through data batches\n",
        "    for idx, batch in enumerate(dataloader):\n",
        "      # print(f\"entered test step {idx} batch loop\")\n",
        "      batch = batch.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      out = model(batch)\n",
        "\n",
        "      # Calculate the loss\n",
        "      loss = loss_fn(out, batch.Class)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Calculate the label predictions\n",
        "      label_preds = torch.argmax(out, dim=1)\n",
        "      # Calculate accuracy\n",
        "      test_acc += (label_preds == batch.Class).sum()/len(label_preds)\n",
        "\n",
        "      # Calculate the AUC\n",
        "      if len(torch.unique(batch.Class)) == 2:\n",
        "        batch_auc = roc_auc_score(batch.Class.detach().cpu().numpy(), out[:,1].detach().cpu().numpy())\n",
        "        test_auc += batch_auc\n",
        "      else:\n",
        "        test_auc += batch_auc\n",
        "\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss/len(dataloader)\n",
        "    test_acc = test_acc/len(dataloader)\n",
        "    test_auc = test_auc/len(dataloader)\n",
        "\n",
        "    return test_loss, test_acc, test_auc"
      ],
      "metadata": {
        "id": "b-oTM34IZeW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average(values:list , window_size:int):\n",
        "    \"\"\"\n",
        "    Calculates the simple moving average of the last window_size elements in a list of values\n",
        "\n",
        "    Returns the average\n",
        "    \"\"\"\n",
        "    if len(values) < window_size:\n",
        "        return None\n",
        "    return sum(values[-window_size:]) / window_size"
      ],
      "metadata": {
        "id": "cp1YPhPFZzrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n",
        "          epochs: int =5,\n",
        "          model_save_path: str = None,\n",
        "          window_size: int=10,\n",
        "          patience: int=10):\n",
        "  \"\"\"\n",
        "  Combines train_step and test_step to train a model and evaluate it at each epoch on the train_dataloader and test_dataloader.\n",
        "\n",
        "  :model_save_path = a string used to save the model's parameters. If no string is provided it is not saved.\n",
        "\n",
        "  Returns a dictionary of results.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # 1. Create empty results dictionary\n",
        "  results = {\"epoch\": [],\n",
        "             \"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"train_auc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": [],\n",
        "             \"test_auc\": [],\n",
        "             \"saved_epochs\": [],\n",
        "             \"test_loss_mov_avg\": [],\n",
        "             \"test_auc_mov_avg\": []}\n",
        "  # 2. Loop through training and testing steps for a number of epochs\n",
        "  best_moving_loss_avg = float('inf')\n",
        "  best_moving_auc_avg = 0\n",
        "\n",
        "  # Loop over the number of epochs\n",
        "  for i in tqdm(range(epochs)):\n",
        "\n",
        "    start_time = timer()\n",
        "    train_step_start_time = timer()\n",
        "    train_loss, train_acc, train_auc = train_step(model,\n",
        "                                       train_dataloader,\n",
        "                                       loss_fn,\n",
        "                                       optimizer)\n",
        "    train_step_end_time = timer()\n",
        "    test_step_start_time = timer()\n",
        "    test_loss, test_acc, test_auc = test_step(model,\n",
        "                                    test_dataloader,\n",
        "                                    loss_fn,\n",
        "                                    optimizer)\n",
        "    test_step_end_time = timer()\n",
        "    # print(f\"Train step time is {train_step_end_time-train_step_start_time:.4f}s, Test step time is {test_step_end_time-test_step_start_time:.4f}s\\n\")\n",
        "\n",
        "    # 3. Print out what's happening\n",
        "    print(f\"Epoch: {i}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Train AUC: {train_auc:.4f}, Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}, Test auc: {test_auc:.4f}\")\n",
        "    # 4. Update results dictionary\n",
        "    append_start_time = timer()\n",
        "    results[\"epoch\"].append(i)\n",
        "    results[\"train_loss\"].append(round(train_loss, 4))\n",
        "    results[\"train_acc\"].append(round(train_acc.item(), 4))\n",
        "    results[\"train_auc\"].append(round(train_auc, 4))\n",
        "    results[\"test_loss\"].append(round(test_loss, 4))\n",
        "    results[\"test_acc\"].append(round(test_acc.item(), 4))\n",
        "    results[\"test_auc\"].append(round(test_auc, 4))\n",
        "    append_end_time = timer()\n",
        "    # print(f\"append time is{append_end_time-append_start_time:.4f}\")\n",
        "\n",
        "    # 5. If model_save_path provided, save the model to its path based on whether test loss and test AUC have improved.\n",
        "    \"\"\"\n",
        "    Once the number of epochs is greater than the window_size a current moving average is created of the last 'window_size'\n",
        "    loss and AUC values. As long as these current metrics are higher than the current moving average, the model is saved.\n",
        "    If the current metrics are not better than the current moving average for 'patience' epochs the training stops early.\n",
        "    \"\"\"\n",
        "\n",
        "    save_timer_start = timer()\n",
        "    if model_save_path:\n",
        "      current_moving_loss_avg = moving_average(results[\"test_loss\"], window_size)\n",
        "      if current_moving_loss_avg is not None:\n",
        "        results[\"test_loss_mov_avg\"].append(round(current_moving_loss_avg, 4))\n",
        "      else:\n",
        "        results[\"test_loss_mov_avg\"].append(None)\n",
        "\n",
        "\n",
        "      current_moving_auc_avg = moving_average(results[\"test_auc\"], window_size)\n",
        "      if current_moving_auc_avg is not None:\n",
        "        results[\"test_auc_mov_avg\"].append(round(current_moving_auc_avg, 4))\n",
        "      else:\n",
        "        results[\"test_auc_mov_avg\"].append(None)\n",
        "\n",
        "      if current_moving_loss_avg is not None and current_moving_auc_avg is not None:\n",
        "        if current_moving_loss_avg < best_moving_loss_avg and current_moving_auc_avg > best_moving_auc_avg:\n",
        "          without_improvement_count = 0\n",
        "          results[\"saved_epochs\"].append(i)\n",
        "          torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
        "                   f=model_save_path)\n",
        "          print(f\"Saved model at epoch {i} with current average test loss: {current_moving_loss_avg:.4f} and previous best: {best_moving_loss_avg:.4f}\")\n",
        "          print(f\"Saved model at epoch {i} with current average AUC loss: {current_moving_auc_avg:.4f} and previous best: {best_moving_auc_avg:.4f}\")\n",
        "          best_moving_loss_avg = current_moving_loss_avg\n",
        "          best_moving_auc_avg = current_moving_auc_avg\n",
        "\n",
        "        else:\n",
        "          without_improvement_count += 1\n",
        "          print(f\"Without_improvement_count: {without_improvement_count}\")\n",
        "        if without_improvement_count > patience:\n",
        "          print(\"Early Stopping\")\n",
        "          break\n",
        "    save_timer_end = timer()\n",
        "    end_time  = timer()\n",
        "    print(f\"Epoch took {end_time-start_time:.2f} seconds\")\n",
        "    # print(f\"Time to save loop : {save_timer_end-save_timer_start:.4f}\")\n",
        "\n",
        "  # 6. Return the filled results at the end of the epochs\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "Ec10XN0Zack5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model_repeats(model_architecture,\n",
        "                      optimizer_,\n",
        "                      criterion,\n",
        "                      models_directory,\n",
        "                      num_hidden_channels,\n",
        "                      pool_method,\n",
        "                      nb_epochs,\n",
        "                      nb_repeats,\n",
        "                      window_size,\n",
        "                      patience):\n",
        "  \"\"\"\n",
        "  Runs training runs for 'nb_repeats' and optionally saves them. Saves the model parameters and model results in\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(nb_repeats):\n",
        "    if model_save_bool:\n",
        "      model_save_name = f\"{i}_{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}.pth\"\n",
        "      model_save_path  = models_directory / model_save_name\n",
        "    else:\n",
        "      model_save_path = None\n",
        "    model = model_architecture()\n",
        "    optimizer = optimizer_(model.parameters())\n",
        "\n",
        "\n",
        "    results = train(model,\n",
        "        train_dataloader,\n",
        "        test_dataloader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        epochs = nb_epochs,\n",
        "        model_save_path = model_save_path,\n",
        "        window_size=window_size,\n",
        "        patience=patience)\n",
        "    with open(models_directory/f\"{i}_{num_hidden_channels}_{nb_epochs}_{pool_method.__name__}_results.pkl\", 'wb') as f:\n",
        "      print(\"Saved results of this model\")\n",
        "      pickle.dump(results, f)"
      ],
      "metadata": {
        "id": "N4sgPv9JcyqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}